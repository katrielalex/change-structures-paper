% Toggle comments for preamble and topmatter to typeset in ACM style

\input{preamble-standard}
%\input{preamble-acm}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{stmaryrd}

\input{notation}

\begin{document}

\input{topmatter-standard}
%\input{topmatter-acm}

\title{Something about change structures}

\maketitle

\section{Introduction}

The paper of Cai et al.~\cite{cai2014changes} defines compelling notions
of change structure and derivative, and use them to provide a composable framework for
evaluating incremental lambda calculus programs provided you have a ``plugin'' for the
primitive operations of your program.

This provides an excellent foundation for using change structures, but there are
many questions remaining about change structures themselves. What are their
algebraic properties? Can we define change structures on more varied kinds of
structures? What are the interactions with orders on the base set?

There are two major motivational targets for this exercise. The first is
lattices. Incremental computation is especially attractive for database or logic
programming languages (Datalog, SQL, etc.), where it can produce drastically
smaller results than naive evaluation. We therefore want to understand
incremental computation in its most general form on lattices, so that we have
maximum flexibility when bringing it to bear on a particular application. For
applications to Datalog in particular, see (cite forthcoming paper, or perhaps
my arxiv preprint).

The calculus of differential geometry is significantly more abstract than real
calculus. The connection to change structures is still tenuous, but there are
telling similarities, and it is our eventual goal to find a sufficiently general
notion of derivative that will encompass differential geometry as well.

The major contributions of this paper are:
\begin{itemize}
  \item A generalised definition of change structures and derivatives
    (\ref{sec:change-structures}).
  \item A description of the algebraic properties of change structures (\ref{sec:algebra}), including
    a definition of function changes that allows them to form a Cartesian closed
    category.
  \item A description of the properties of change structures over posets
    (\ref{sec:order}), including theorems that characterize the range of
    possible derivatives available, and conditions under which compound change
    structures may admit derivatives.
\end{itemize}

\section{Change structures}
\label{sec:change-structures}

MARIO: Should we pick a different name to distinguish them from Cai-style change structures?
MPJ: Not sure. I do really like ``change structure'', and I think they're
similar, but perhaps we should use a different name. Forward algebra? Change action?
MARIO: I've been calling them ``forward algebras'', but the specific name is not particularly
important. I would definitely like to have a different name, especially since we often contrast the two.
\begin{defn}[Change structures]

  A \textit{change structure} is defined as:

  $$\mathcal{A} \defeq \cstruct{A}{\changes{A}}{\cplus}$$

  where $A$ is an object in some category $\cat{C}$, $\changes{A}$ is a semigroup, and $\cplus$ gives a semigroup action on $A$.

  We will call $A$ the base set (although it may not be a set in general), and $\changes{A}$ the change set of the change structure.
\end{defn}

Elements in the change set represent changes that can be made to elements in the
base set, with the semigroup action being the operation that ``applies'' the
change. The requirement that the change set be a semigroup is convenient but in
fact inessential: given any set with an action on the base set, we can take the
free semigroup over the action set to obtain a semigroup action (in fact, we can
easily extend to a monoid as well).

The fact that the change set is a semigroup action reveals the heart of the
change structure definition: the change set is a representation of a
\textit{transformation semigroup} over the base set, with the semigroup
operation simply being composition.

Here are some recurring examples of changes structures:
\begin{itemize}
  \item $A_\discrete \defeq \cstruct{A}{\emptyset}{\emptyset}$, the discrete change structure on any base set.
  \item $A_\Rightarrow \defeq \cstruct{A}{A\Rightarrow A}{ev}$, where $A \Rightarrow A$ denotes the exponential object.
  \item $A_\leq \defeq \cstruct{A}{\leq \subseteq A \times A}{ev_\leq}$, where $\leq$ is some transitive relation and $ev_\leq$ denotes
    conditional application, i.e. $evl_\leq(a, (b, c))$ is equal to $c$ if $a = b$, and $a$ otherwise. Composition of changes is obtained
    by transitivity of $\leq$.
  \item $\mathbb{Z}_1 \defeq \cstruct{\mathbb{Z}}{\mathbb{N}}{+}$
  \item $\mathbb{Z}_2 \defeq \cstruct{\mathbb{Z}}{\mathbb{N}}{-}$
  \item $\mathbb{Z}_3 \defeq \cstruct{\mathbb{Z}}{\mathbb{Z}}{+}$
  \item $F_2 \defeq$ integers modulo 2 with addition.
\end{itemize}
Indeed, any semigroup $(A, \splus)$ can be seen as a change structure $\cstruct{A}{A}{\splus}$. In particular,
the change set of any change structure is itself a change structure.

Given change structures and functions between them, we have a natural notion of a derivative, following \cite{cai2014changes}:

\begin{defn}[Derivatives]
  A \textit{derivative} of a function $f: A_\cplus \rightarrow B_\cpluss$ is a function $\derive{f}: A \times \changes{A} \rightarrow
  \changes{B}$ such that
  $$f(a \cplus \change{a}) = f(a) \cpluss \derive{f}(a, \change{a})$$

  A function which has a derivative is called \textit{differentiable}.
\end{defn}

Derivatives need not be unique, in general, so we will speak of ``a''
derivative. 

\begin{thm}[The Chain Rule]
  Let $f: A_\cplus \rightarrow B_\cpluss$, $g: B_\cpluss \rightarrow C_\cplusss$ be differentiable functions. Then $g \circ f$ is also
  differentiable, with derivative given by
   $$\derive{(g \circ f)}(x, \change{x}) = \derive{g}\left(f(x), \derive{f}(x, \change{x})\right)$$
   or, in curried form
   $$\derive{(g \circ f)}(x)(\change{x}) = \derive{g}(f(x)) \circ \derive{f}(x)$$
\end{thm}
\begin{proof}
  By equivalence:
  \begin{itemize}
    \item[ ]$(g \circ f)(x) \cplusss \derive{g}\left(f(x), \derive{f}(x,\change{x})\right)$
    \item[=]$(g(f(x)) \cplusss \derive{g}\left(f(x), \derive{f}(x,\change{x})\right)$
    \item[=]$g\left(f(x) \cpluss \derive{f}(x, \change{x}) \right)$
    \item[=]$g\left(f(x \cplus \change{x})\right)$
    \item[=]$(g \circ f)(x \cplus \change{x})$
  \end{itemize}
  Therefore $\derive{g}\left(f(x), \derive{f}(x, \change{x})\right)$ is a
  derivative for $(g \circ f)$.
\end{proof}

Why call this the ``chain rule''? It doesn't have quite the same structure as
the chain rule in real calculus ($\derive{(g \circ f)}(x) = (\derive{g} \circ f)
(x) \cdot \derive{f}(x)$), but it does have the same structure as the chain rule
from differential geometry ($\derive{(g \circ f)}(x, \textbf{v}) = \derive{g}
(f(x), \derive{f}(x, \textbf{v}))$), which will turn out to be a recurring connection.

\subsection{Minus operators and completeness}

Cai et al.~\cite{cai2014changes} include a ``minus operator'' in their definition of change structures. 

\begin{defn}[Minus operator]
  A \textit{minus operator} is a function $\cminus: A \times A \rightarrow \changes{A}$ such that $a \cplus (b \cminus a) = b$.
\end{defn}

We have omitted minus operators from our definition because
there are many interesting change structures that do not have them (for example,
$\mathbb{Z}_3$ does, but $\mathbb{Z}_1$ does not).

\begin{defn}[Completeness]
  A change structure is \textit{complete} if for any $a, b \in A$, there is
  a change $\change{a} \in \changes{A}$ such that $a \cplus \change{a} = b$.
\end{defn}

\begin{prop}[Completeness equivalences]
  Let $A$ be a change structure. Then the following are equivalent:
  \begin{itemize}
    \item $A$ is complete.
    \item The semigroup action is transitive.
    \item There is a minus operator on $A$.
  \end{itemize}
\end{prop}

\begin{defn}[Minus derivative]
  Given a minus operator $\cminus$, we have a derivative for any function $f$,
  defined as
  $$\derive{f}_\cminus(a, \change{a}) \defeq f(a \cplus \change{a}) \cminus f(a)$$
\end{defn}

\subsection{Extensionality}

MPJ: I think this should have a different name (I guess we could use ``free'') -
extensionality makes me think of something like ``the element is characterized
by its action''.
However, that's probably not a very interesting notion, since it actually gives us a
semigroup congruence, so we can always quotient out by it.
MARIO: Agreed, I've been calling it ``tightness'' (in the sense that the change set
is as small as it can be), but I don't really like the name, so I'm open to any
alternatives.
\begin{defn}[Extensionality]
  A change structure is \textit{extensional} if whenever $a \cplus \change{a}
  = a \cplus \change{b}$ for some $a \in A$, it is the case that $\change{a} = \change{b}$.
\end{defn}

Many change structures are not extensional, for example $F_2$.

\begin{prop}[Extensionality equivalences]
  Let $A$ be a change structure. Then the following are equivalent.
  \begin{itemize}
    \item $A$ is extensional.
    \item The semigroup action is free.
    \item The equalizer of any two elements is empty.
  \end{itemize}
\end{prop}

Extensionality gives us uniqueness of derivatives:
\begin{prop}
  Let $B$ be an extensional change structure, and $f: A \rightarrow B$. Then $f$ has at
  most one derivative.

  Conversely, if $\textrm{id}: B \rightarrow B$ has at most one derivative, then
  $B$ is extensional.
\end{prop}

\section{Algebra of change structures}
\label{sec:algebra}

\subsection{Categorical constructions}

\begin{defn}[Category of change structures]
  We define the category $\cat{CStruct}$ of change structures over objects in
  some category $\cat{C}$. The objects are
  change structures and the morphisms are differentiable functions. We denote
  the set of differentiable functions between $A$ and $B$ as $A \difffunc B$.
  
  We will usually leave the category $\cat{C}$ implicit, or refer to it as the
  underlying category.
\end{defn}

We can now see the difference between $\cat{CStruct}$ and the category of
S-acts $\cat{SAct}$: the objects of $\cat{SAct}$ all maintain the same monoid
structure, whereas we are interested in changing the structure of the act.

However, if we compare the definition of a $\cat{SAct}$ ``act-preserving''
homeomorphism (as in \cite{kilp2000monoids}) we can see that the structure is quite similar to our definition
of differentiability:

$$f(a \splus s) = f(a) \splus s$$

as opposed to

$$f(a \cplus s) = f(a) \cplus \derive{f}(a, s)$$

That is, we use $\derive{f}$ to transform the action element into the new
structure, whereas in $\cat{SAct}$ it simply remains the same.

In fact, $\cat{SAct}$ is a subcategory of $\cat{CStruct}$, where we only
consider change structures with change set $S$, and the only functions are those
whose derivative is $\lambda a. \lambda d. d$.

\begin{prop}[Products]
  Let $A = \cstruct{A}{\changes{A}}{\cplus}$ and $B =
  \cstruct{B}{\changes{B}}{\cpluss}$ be change structures, and suppose that the
  underlying category has products.

  Then $A \times B \defeq \cstruct{A \times B}{\changes{A} \times
  \changes{B}}{\cplus \times \cpluss}$ is their categorical product.
\end{prop}
\begin{proof}
  Let $Y$ be a change structure, and $f_1: Y \rightarrow A$, $f_2: Y
  \rightarrow B$ be morphisms.

  Then the product morphism in the underlying category, $f_1 \times f_2$ is the product
  morphism in $\cat{CStruct}$.

  First, we show it is a morphism (i.e.) is differentiable. It can easily be
  shown that $\derive{f_1} \times \derive{f_2}$ is a derivative of $f_1 \times f_2$.

  Commutativity and uniqueness follow from the corresponding properties of the
  product in the underlying category.

  MARIO: This is actually a bit trickier, since we also need to show that e.g. 
  $(A \times B, \discrete)$ is not a product. We should also mention that any change
  structure with the same reachability order as $\Delta A \times \Delta B$ is a possible
  product (since they are isomorphic).
  MPJ: ugh, yes, of course there are more change structures than sets, so
  we need to distinguish among them here and below. Agree that it will generally
  be the biggest/smallest change structure on the thing though.
\end{proof}

\begin{prop}[Equalizers]
  Let $A$ and $B$ be change structures, $f, g: A \rightarrow B$ be morphisms
  between them, and suppose that the underlying category has equalizers.

  Let $\equalizer{f}{g}$ be the equalizer of $f$ and $g$ in the underlying category.

  Then $\equalizer{f}{g}_\discrete$ is an equalizer for $f$ and $g$
  in $\cat{CStruct}$.

  MARIO: I don't think this is quite true, this would entail that the equalizer of
  $f : A, \Delta A \rightarrow B,\Delta B$ and itself is $A, \discrete$ rather than
  $A, \Delta A$ - I do think equalizers exist but $\Delta\equalizer{f}{g}$ is the
  `largest' change structure that still makes $\equalizer{f}{g}$ differentiable (there
  is no unique choice for this)

  MARIO: Actually, is there a more general version of this? E.g. Whenever the underlying
  category has limits, the limit in FAlg is the ``largest'' fwd-algebra that makes the arrows
  involved differentiable / same for colimits with ``smallest''. Is there a categorical definition
  of ``smallest/largest''? A limit/colimit on the category of fwd-algebas over the specific object?
\end{prop}
\begin{proof}
  The equalizer morphism is differentiable, since any function from a discrete change
  structure is differentiable (see below), so it is a valid morphism in $\cat{CStruct}$.

  Equalization and uniqueness follow from the corresponding properties of the
  equalizer in the underlying category.
\end{proof}

\begin{thm}
  $\cat{CStruct}$ has all finite limits if the underlying category does.
\end{thm}

\begin{prop}[Exponentials]
  Let $A_\cplus$ and $B_\cpluss$ be change structures, and suppose that the
  underlying category has exponentials.

  Then $\cstruct{A \difffunc B}{A
    \rightarrow \changes{B}}{\lambda d. \lambda f. \lambda a. f(a) \cpluss
    d(a)}$ is a change structure on $A \difffunc B$ and the exponential object.

  The semigroup structure on $A \rightarrow \changes{B}$ is the semigroup
  structure on $\changes{B}$ lifted pointwise, so we will typically reuse the
  change operator for $B$ for $A \rightarrow \changes{B}$.
\end{prop}
\begin{proof}
  We need to show that the evaluation map $\ev: (A \difffunc B) \times A
  \rightarrow B$ is differentiable, the other properties follow from the
  properties of the exponential object in $\cat{Set}$.

  TODO: I actually have no idea if this is even right. We definitely need to
  talk about function changes at some point, regardless.

  MARIO: I have only been able to prove this assuming the change structures are
  commutative and extensional. I think it's easy to relax extensionality, but
  commutativity is actually vital (actually nevermind, commutativity is only necessary
  to prove $\ev$ is smooth, not differentiable).

  MARIO: At any rate, it would be good if we can find a justification for our
  function changes that isn't this (or the geometric hand-waving, or just saying
  Cai's is obviously bad - even though it is!)
\end{proof}

\begin{prop}[Incrementalization]
\label{prop:incrementalization}
  The derivative of $\ev: (A \difffunc B) \times A \rightarrow B$ is 
  $$\derive{\ev}((f, a), (\change{f}, \change{a})) = \derive{f}(a, \change{a}) \splus \change{f}(a \cplus \change{a})$$
  Therefore 
  $$(f \cplus \change{f})(a \cplus \change{a}) = f(a) \cplus \left[\derive{f}(a,
    \change{a}) \splus \change{f}(a \cplus \change{a}) \right]$$
\end{prop}
MARIO: nice proof, I'm a bit worried about the $f'$, though, since there may be more than one,
and I feel like we need the axiom of choice to ``pick'' a $f'$ for every $f$.

This is our equivalent of Cai et al.'s ``Incrementalization'' lemma.

TODO: Can I be reminded of why we don't have coproducts? Extensional forward algebras
don't, but the argument doesn't apply to arbitrary ones, I think.

\begin{thm}
  $\cat{CStruct}$ is a Cartesian closed category if the underlying category is.
\end{thm}

\subsection{Ordering change structures}

We can put an order on the change structures for a given base set as follows:

\begin{defn}[Change structure ordering]
  $A_\cplus \fineOrder A_\cpluss$ iff $\textrm{id}: A_\cplus \rightarrow A_\cpluss$ is differentiable.
\end{defn}

Transitivity of the order follows from the chain rule, and reflexivity is trivial.

This ordering is useful because it gives us a natural sense of the ``fineness''
of a change structure, much like the corresponding version in topology.

\begin{prop}
  If $f: A_\cplus \rightarrow B_\cpluss$ is differentiable, then
  \begin{itemize}
    \item if $A_\cplusss \fineOrder A_\cplus$ then $f: A_\cplusss \rightarrow
      B_\cpluss$ is differentiable.
    \item if $B_\cplus \fineOrder B_\cplusss$ then $f: A_\cplus \rightarrow
      B_\cplusss$ is differentiable.
  \end{itemize}
\end{prop}

That is, functions remain differentiable if the source change structure becomes
coarser, or the target change structure becomes finer (again, mirroring topology).

Furthermore, $\fineOrder$ also gives a fineness ordering on the reachability orders.

\begin{prop}
  If $a \reachOrder b$ in $A_\cplus$ and $A_\cplus \fineOrder A_\cpluss$, then $a \reachOrder b$ in $A_\cpluss$.
\end{prop}

\subsection{Superpositions}

As well as combining change structures entire, we can combine two different
change structures on the same underlying set.

\begin{defn}[Superposition]
  Let $A_\cplus = \cstruct{A}{\changes{A}_\cplus}{\cplus}$ and $A_\cpluss =
  \cstruct{A}{\changes{A}_\cpluss}{\cpluss}$ be change structures.

  Then the \textit{superposition} of $A_\cplus$ and $A_\cpluss$ is defined as:
  $$A_\cplus \superpose A_\cpluss \defeq \cstruct{A}{
    (\changes{A}_\cplus \times \changes{A}_\cpluss)^\ast}{\star^\ast}$$

  where $X^\ast$ is the set of finite sequences of $X$, and $a \star (d, e)
  \defeq a \cplus d \cpluss e$.
\end{defn}

Here we have used the ``trick'' mentioned earlier to ensure that our change
set has a semigroup structure: $\cstruct{A}{(\changes{A}_\cplus \times
  \changes{A}_\cpluss)}{\star}$ does not have a semigroup structure, so we take
the free extension to finite sequences.

In some cases we may be able to find a more compact representation of the
superposition change structure, for example $\mathbb{Z}_1 \superpose \mathbb{Z}_2$ is isomorphic to $\mathbb{Z}_3$.

TODO: this isn't quite right, I think it's a coproduct only on the category of algebras over some fixed 
set $A$. Should we mention this in the paper?
Superposition is a useful construction, because it is a (weak) coproduct.

\begin{prop}
  $A \superpose B$ is a weak coproduct of $A$ and $B$.
\end{prop}
\begin{proof}
  TODO - also, I think we may need the component change structures to actually
  be monoidal here, so that we can lift $\change{b}$ to $(0, \change{b})$ in
  the superposition.
  MARIO: either this or define the superposition by $(\changes{A}_\cplus + \changes{A}_\cpluss)^\ast$,
  which should be equivalent, I think.
\end{proof}

However, the weakness is not terribly important when considering $\fineOrder$
(in particular, if we drop to the category corresponding to that order, the
coproduct is strong again, since there is at most one morphism between any two
objects). 

\begin{corollary}
  $A_\cplus \superpose A_\cpluss$ is the least upper bound of $A_\cplus$ and $A_\cpluss$ with respect to $\fineOrder$.
\end{corollary}

That is, the superposition is the weakest change structure that is finer than both
$A_\cplus$ and $A_\cpluss$.

This gives us a join-semilattice for change structures on a given set.

\begin{thm}[Change structure semilattice]
  Change structures on a base set $A$ form a bounded join-semilattice 
  ordered by $\fineOrder$, with the least element given by
  $A_\discrete$, and the join operation given by $\superpose$.
\end{thm}

All functions into a complete change structure are differentiable, so any
complete change structures on $A$ will be maximal elements of the lattice, while
the discrete change structure provides a minimal element.

\section{Change structures over other structures}
\label{sec:moreStructures}

\subsection{Posets}
\subsubsection{Orders from change sets}

Whenever the change set is a monoid, 
there is a natural preorder on the base set of a change structure, given by reachability
 under the action:
\begin{defn}[Reachability order]
  $a \reachOrder b$ iff there is a $\change{a} \in \changes{A}$ such that $a \cplus
  \change{a} = b$.
\end{defn}

The reachability order of a complete change structure on $A$ is precisely the trivial relation
$A \times A$. Conversely, any such change structure is necessarily complete.

\begin{prop}
  A function is differentiable iff it is monotonic with respect to the
  reachability order.
\end{prop}

\begin{corollary}
  Any function from a discrete change structure or into a complete change
  structure is differentiable.
\end{corollary}

Conversely, any preorder $\leq$ on some set $A$ induces the corresponding change structure
$A_\leq$.
TODO: I suspect this forms a functor which is right adjoint to the forgetful functor into $\cat{PreOrd}$.
Conversely, I think there is a left adjoint which maps $A, \leq$ to $A, A \Rightarrow_\leq A$ ($\Rightarrow_\leq$
denoting the set of $\leq$-monotone functions) - the isomorphism of Hom-sets is obvious, but I can't prove naturality.

\subsubsection{Orders on base sets}

A common structure which we want to compute changes on is a poset. For this
section we shall assume that all of our base sets are posets.

Firstly, we can define ``approximations'' to derivatives from both sides.

\begin{defn}
  Let $f: A_\cplus \rightarrow B_\cpluss$ be a function. Then a \textit{sup-derivative}
  of $f$ is a function $\supderive{f}$ such that
  $$f(a \cplus \change{a}) \leq f(a) \cpluss \supderive{f}(a, \change{a})$$
  
  Similarly, a \textit{sub-derivative} of $f$ is a function $\subderive{f}$ such that 
  $$f(a \cplus \change{a}) \geq f(a) \cpluss \subderive{f}(a, \change{a})$$

  A function with a sup-derivative is sup-differentiable, and a function with a
  sub-derivative is sub-differentiable.
\end{defn}

Both sup- and sub-derivatives satisfy the chain rule, in the following sense: 
\begin{prop}[Chain rule for sub-derivatives]
  let $f : A_\cplus \rightarrow B_\cpluss$ and $g : B_\cpluss \rightarrow C_\cplusss$ be
  sub-differentiable functions with sub-derivatives $f', g'$ respectively. Then
  $$\derive{g}(f(a)) \circ \derive{f}(a)$$ is a subderivative of $g \circ f$
\end{prop}

Some change structures always have sub- or sup-derivatives: for example every function
into $\mathbb{Z}_1$ is sup-differentiable, and every function into $\mathbb{Z}_2$ is 
sub-differentiable.

\begin{prop}
  If $\derive{f}$ is both a sub- and sup-derivative of $f$, then it is a derivative of $f$.
\end{prop}

Note that this is not the same as saying that if $f$ is both sub- and
sup-differentiable, then it is differentiable. The functions which provide the
sub- and sup-derivatives must coincide for that to be the case. For example,
consider any bounded lattice with the change set $\{ \lambda k . \bot, \lambda k
 . \top \}$. Then any function has a sub-derivative (mapping everything to
 bottom), and a sup-derivative (mapping everything to top), but in most cases
 these are not true derivatives.

Secondly, if the base set of a change structure is a poset, then this gives us a natural
order on the change set.

\begin{defn}[Change order]
  $\change{a} \changeOrder \change{b}$ iff for all $a \in A$ it is the case that $a \cplus \change{a} \leq a \cplus \change{b}$.
\end{defn}

Alternatively, the change order is the largest (TODO: I think this is right?) order such that $\cplus$ is monotonic with
respect to its second argument.

If the change structure is extensional, then the order is antisymmetric, and a
full partial order.

Having a monotonic order on the changes is very useful.

\begin{thm}
  Let $f: A \rightarrow B$ be a function, and let $\changeOrder$ be a preorder on $\changes{B}$ such that $\cplus$ is monotonic with
  respect to it. Then let $\supderive{f}$ be a sub-derivative for $f$, and $h: A \times
  \changes{A} \rightarrow \changes{B}$ be a function such that
  $$\supderive{f} \changeOrder h$$
  Then $h$ is also a sup-derivative for $f$.

  Similarly, if $\subderive{f}$ is a sup-derivative for $f$ such that 
  $$h \changeOrder \subderive{f}$$
  Then $h$ is also a sub-derivative for $f$.
\end{thm}
\begin{proof}
  We prove the first case:
  \begin{itemize}
    \item[ ]$\supderive{f}(a, \change{a}) \changeOrder h(a, \change{a})$
    \item[$\Rightarrow$]\{ by monotonicity \}\\
      $f(a) \cplus \supderive{f}(a, \change{a}) \leq f(a) \cplus h(a, \change{a})$
    \item[$\Rightarrow$]\{ sup-derivative property \}\\
      $f(a \cplus \change{a}) \leq f(a) \cplus h(a, \change{a})$
  \end{itemize}

  The proof for the other case is symmetric.
\end{proof}

\begin{thm}[Sandwich lemma]
  \label{thm:sandwich}
  Let $\supderive{f}$ be a sup-derivative for $f$, $\subderive{f}$ be a sub-derivative for $f$, $\changeOrder$ be a preorder on $\changes{B}$ such that $\cplus$ is monotonic with
  respect to it, and $g$ be such that

  $$\supderive{f} \changeOrder g \changeOrder \subderive{f}$$

  Then $g$ is a derivative for $f$.
\end{thm}

In particular, this applies if $g$ and $h$ are themselves derivatives. Moreover,
although the condition of the theorem only requires the bounds to be sub- and
sup-derivatives, the conclusion of the theorem also applies to the bounds, so
they will always be full derivatives as well.

\subsubsection{Ascending and descending change structures}

Sub- and sup-derivatives alone are not quite enough to allow us to make
functions fully differentiable. We need some additional power.

\begin{defn}[Ascending and descending change structures]
  A change structure $A$ is \textit{ascending} if $a \leq b$ implies $a
  \reachOrder b$.

  A change structure $A$ is \textit{descending} if $a \leq b$ implies $b
  \reachOrder a$.
\end{defn}

Intuitively, an ascending change structure is one where you can produce
arbitrary changes that follow the partial order.

\begin{corollary}
  A change structure which is both ascending, descending, and has a minimal or
  maximal element is complete.
\end{corollary}

\begin{thm}
  Let $A$, $B$ be a change structures, $f: A \rightarrow B$ be a function. Then
  any of the following are sufficient for $f$ to be differentiable.
  \begin{itemize}
    \item $B$ is ascending, and $f$ is sub-differentiable.
    \item $B$ is descending, and $f$ is sup-differentiable.
    \item $B$ is ascending, descending, and has a minimal or maximal element.
  \end{itemize}
\end{thm}

In particular, we can construct change structures where functions are
differentiable by superposing multiple change structures that have some of the
properties that we want.

For example, $\mathbb{Z}_1$ is ascending (and has sup-derivatives), and
$\mathbb{Z}_2$ is descending (and has sub-derivatives), so $\mathbb{Z}_1
\superpose \mathbb{Z}_2 = \mathbb{Z}_3$ has derivatives.

\subsubsection{Maximal and minimal derivatives}

TODO: this section isn't very elegant

If we have a minus operator, then our change structure is complete and all
functions are differentiable. However, there may still be multiple derivatives
for a given function, and we can distinguish them using our order on the change
set.

\begin{defn}[Minus ordering]
  $\cminus_1 \minusOrder \cminus_2$ iff for all $a,b \in A$, $a \cminus_1 b
  \changeOrder a \cminus_2 b$.
\end{defn}

This orders our minus operators according to the size of the changes they
produce. 

\begin{prop}
  If $\cminus_1 \minusOrder \cminus_2$ then
  $\derive{f}_{\cminus_1} \changeOrder \derive{f}_{\cminus_2}$.
\end{prop}

\begin{prop}
  If $\cminus$ is a minimal (maximal) minus operator, then $\derive{f}_\cminus$
  is a minimal (maximal) derivative.
\end{prop}

This then gives us a full characterisation of the derivatives on a complete
change structure.

\begin{thm}[Characterization of derivatives]
\label{thm:derivativeCharacterization}
  Let $A$ be a change structure and $B$ be a complete change structure, let
  $f: A \rightarrow B$ be a function, and let $\subderiveM{f}$ and
  $\supderiveM{f}$ be minimal and maximal derivatives of $f$, respectively.
  Then the derivatives of $f$ are precisely
  the functions $\derive{f}$ such that
  $$\subderiveM{f} \changeOrder \derive{f} \changeOrder \supderiveM{f}$$
\end{thm}
\begin{proof}
  Follows easily from \ref{thm:sandwich} and minimality/maximality.
\end{proof}

This theorem gives us leeway when trying to pick a derivative: we can pick out the
bounds, and that tells us how much ``wiggle room'' we have. This is helpful
because some of the intermediary functions may be much easier to compute than
others, or convenient for other reasons.

\subsection{Lattices}

\begin{defn}
  Let $L$ be a join-semilattice. Then $L_\vee \defeq \cstruct{L}{L}{\vee}$ is a change
  structure on $L$.

  Similarly, if $L$ is a meet-semilattice, then $L_\wedge \defeq \cstruct{L}{L}{\wedge}$ is a change
  structure on $L$.
\end{defn}

\begin{prop}
  All of the following hold
  \begin{itemize}
    \item $L_\vee$ is ascending and has sup-derivatives.
    \item $L_\wedge$ is descending and has sub-derivatives.
    \item $L_\vee \superpose L_\wedge$ has derivatives.
  \end{itemize}
\end{prop}

As usual, the superposition change structure is a pain to work with, since it
consists of sequences of ``upwards'' and ``downwards'' changes. However, it does
give us a complete change structure for any lattice.

\subsection{Boolean algebras}

Boolean algebras give us a much more compact representation for the
superposition of $L_\vee$ and $L_\wedge$.
TODO: Can we do this for lattices? Heyting algebras?

\begin{prop}
  Let $L$ be a Boolean algebra. Define
  $$L_\superpose \defeq \cstruct{L}{L \times L}{\twist}$$
  where
  $$a \twist (p, q) \defeq (a \vee p) \wedge \neg q$$
  and the semigroup action is
  $$(p, q) \splus (r, s) \defeq ((p \wedge \neg s) \vee r, (q \vee s) \wedge \neg r)$$

  Then $L_\superpose$ is isomorphic to $L_\vee \superpose L_\wedge$.
\end{prop}

We can think of $L_\superpose$ as tracking changes as pairs of ``upwards'' and
``downwards'' changes, where the semigroup action simply applies both.

Boolean algebras also have clean definitions for maximal and minimal minus
operators.

\begin{prop}
  Let $L$ be a Boolean algebra. Then
  $$a \cminus_\bot b = (a \wedge \neg b, b)$$
  $$a \cminus_\top b = (a, b \wedge \neg a)$$

  define minimal and maximal minus operators.
\end{prop}

In particular, \ref{thm:derivativeCharacterization} gives us clear bounds for
all the derivatives on Boolean algebras:

\begin{corollary}
\label{cor:booleanCharacterization}
  Let $L$ be a Boolean algebra with the $L_\superpose$ change structure, $A$ be
  a change structure, and $f: A \rightarrow
  L$ a function. Then the derivatives of $f$ are precisely those functions
  $\derive{f}$ such that
  $$
  (
    f(a \cplus \change{a}) \wedge \neg f(a), 
    f(a)
  )
  \changeOrder
  \derive{f}(a, \change{a})
  \changeOrder
  (
    f(a \cplus \change{a}), 
    f(a) \wedge \neg f(a \cplus \change{a})
  )
  $$
\end{corollary}

\section{Fixpoints}

\subsection{Incremental computation of fixpoints}

Derivatives give us a technique for computing fixpoints incrementally. Kleene's
fixpoint theorem tells us that fixpoints exist for monotone functions on dcpos, and also gives us
a simple procedure for computing them: start from $\bot$ and apply the function
until there is no change. However, this can be woefully inefficient.

In the Datalog literature, the approach of computing the fixpoint by bottom-up
iteration is called ``naive evaluation''. Naive evaluation has the property that
if it derives a fact at some iteration, it will derive that fact at each
subsequent iteration as well. This is obviously wasteful, and can turn what
should be a linear computation into a quadratic one.

The canonical solution to this problem is ``semi-naive evaluation'', which
attempts to derive only the new facts at each iteration. However, ``semi-naive''
as traditionally presented has some warts, and
the following theorem provides a generalization of it to any differentiable function over a
change structure. We will see the details of how to differentiate Datalog
semantics in section \ref{sec:datalog}.

\begin{thm}[Incremental computation of iterated function applications]
\label{thm:diffIter}
  Let $L$ be a dcpo with least element $\bot$ and a change structure, $f: L \rightarrow L$ be
  differentiable. Define $F_i$ as follows:

  \begin{eqnarray*}
  F_0 & = & x\\
  \Delta F_0 & = & (\bot, \top) \\
  F_1 & = & f(F_0)\\
  \Delta F_1 & = & (F_1, F_1) \footnotemark \\
  F_{i+2} & = & F_{i+1} \cplus \Delta F_{i+2} \\
  \Delta F_{i+2} & = & \derive{f}(F_i, \Delta F_{i+1}) \\
  \end{eqnarray*}

  Then 
  $$f^i(x) = F_i$$
\end{thm}

\footnotetext{A number of different values will do here, in general one can use $F_1
\cminus F_0$ given a valid minus operator.}

\begin{proof}
We proceed inductively, proving that $F_{i+2} = f(F_{i+1})$

\begin{itemize}
\item[ ]$F_{i+2}$
\item[=]
$
F_{i+1} \cplus \Delta F_{i+2}
$
\item[=]
$
F_{i+1} \cplus \derive{f}( F_i, \Delta F_{i+1})
$
\item[=] \{ by induction \}\\
$
f(F_i) \cplus \derive{f}(F_i, \Delta F_{i+1})
$
\item[=]
$
f(F_i \cplus \Delta F_{i+1})
$ 
\item[=]
$f(F_{i+1})$
\end{itemize}
\end{proof}

\begin{corollary}[Differential computation of fixed points]
\label{corollary:diffFP}
  Fixed points of differentiable functions which can be calculated by repeated
  function application can also be calculated incrementally.
\end{corollary}

\subsection{Derivatives of fixpoints}

The previous section has shown us how to use derivatives to compute fixpoints
more efficiently, but we might also want to take the derivative of a fixpoint
itself. The typical case for this will be where we have some fixed point
$$\fixpoint (\lambda X . F(E, X))$$
and we now wish to apply a change to $E$ and compute
$$\fixpoint (\lambda X . F(E \cplus \change{E}, X))$$

That is, we are applying a change to the function whose fixpoint we are taking.

In Datalog this would allow us to update a recursively defined relation given an
update to a non-recursive dependency of it, such as the base database. In the
traditional databases literature, this amounts to a solution to the ``recursive
view update problem''.

However, this requires us to have a derivative for the fixpoint operator $\fixpoint$.

\begin{thm}[Derivatives of fixpoints]
\label{thm:fixpointDiff}
  Let 
  \begin{itemize}
    \item$\fixpoint_X : (X \rightarrow X) \rightarrow X$ be a fixpoint operator
    \item $A$ be a change structure
    \item $f: A \rightarrow A$ be a differentiable function
    \item $\change{f} \in \changes{(A \rightarrow A)}$ be a change to $f$
  \end{itemize}

  Suppose a change $\change{w} \in \Delta A$ satisfies
  the equation:
  \begin{equation}\label{eqn:fixcondition}
    \fixpoint_A(f) \cplus \change{w} = \fixpoint_A(f) \cplus \derive{\ev}((f, \fixpoint_A(f)), (\change{f}, \change{w}))
  \end{equation}
  Then $\fixpoint_A(f) \cplus \change{w}$ is a fixpoint of $f \cplus \change{f}$. Conversely
  if $\fixpoint_A(f) \cplus \change{w}$ is a fixpoint of $f \cplus \change{f}$ then
  it satisfies equation \ref{eqn:fixcondition}.
  
  In particular, this means that 
  if the operator $\fixpoint_X$ is defined over $\Delta A$, we can define:
  $$
  \derive{\fixpoint_A}(f, \change{f}) \defeq
  \fixpoint_{\changes{A}}(
    \lambda \change{} .
      \derive{\ev}((f, \fixpoint_A(f)), (\change{f}, \change{}))
  )
  $$
  thus $\fixpoint_A(f) \cplus \derive{\fixpoint_A}(f, \change{f})$ is a fixpoint 
  of $f \cplus \change{f}$.\\

  Furthermore, if
  \begin{itemize}
    \item $A$ is a partial order
    \item $\fixpoint$ computes least fixpoints and is differentiable
    \item $\cplus$ is monotonic with respect to the order on $\changes{A}$
  \end{itemize}
  then the previous expression is also a least fixpoint and therefore $\derive{\fixpoint_A}$
  is a derivative of $\fixpoint_A$.
\end{thm}

\newcommand{\thefixpoint}{F}
\newcommand{\theadjustment}{\operatorname{adjust}}

\begin{proof}
  Throughout, let $\thefixpoint = \fixpoint_A(f)$.

  For the first part, let $\change{w} \in \Delta A$ satisfy equation \ref{eqn:fixcondition}.
  \begin{itemize}
  \item[ ]
    $
    (f \cplus \change{f})(\thefixpoint \cplus \change{w})
    $
  \item[=]\{ by \ref{prop:incrementalization} \}\\
    $
    f(\thefixpoint)
    \cplus
    \derive{\ev}((f, \fixpoint_A(f)), (\change{f}, \change{w}))
    $
  \item[=]\{ rolling the fixpoint and equation \ref{eqn:fixcondition} \}\\
    $
    \thefixpoint
    \cplus
    \change{w}
    $
  \end{itemize}
  Hence $\thefixpoint \cplus \change{w}$ is a fixpoint of $f \cplus \change{f}$. The converse
  follows from reversing the direction of the proof.

  For the second part, we are supposing that $\fixpoint$ computes least
  fixpoints and is differentiable, and $\cplus$ is monotonic in its second argument.

  Then suppose that $D$ is a derivative of $\fixpoint_X$, and hence $\thefixpoint \cplus D(f,
  \change{f})$ is the least fixpoint of $f \cplus \change{f}$. 
  Since $D$ is a derivative of $\fixpoint_X$, it follows that for every $f, \change{f}$ the 
  equation \ref{eqn:fixcondition} is satisfied, i.e. for every $f, \change{f}$
  \begin{align*}
    \fixpoint_A (f \cplus \change{f})
    &= \fixpoint_A(f) \cplus D(f, \change{f})\\
    &= \fixpoint_A(f) \cplus ev'((f, \fixpoint_A(f)), (\change{f}, D(f, \change{f}))
  \end{align*}

  MARIO: WARNING: what follows is not constructive and very unsatisfactory.

  Then the map $ev(f, a)$ admits a derivative $ev'_D((f, a), (\change{f}, \change{a}))$ that
  coincides with $D$ whenever $a = \fixpoint_A(f), \change{a} = D(f, \change{f})$. Hence
  $D(f, \change{f})$ is a fixpoint of the function
  $$
    \omega(\change{w}) \defeq ev'_D((f, \fixpoint_A(f)), (\change{f}, \change{w}))
  $$
  Now let 
  $$
  \change{w} \defeq \fixpoint_\changes{A}(\omega)
  $$
  We have $\fixpoint_A(f) \cplus \change{w}$ is a fixpoint of $f \cplus \change{f}$. Furthermore,
  since $\change{w}$ is, by definition, a least fixpoint, we have $\change{w} \leq D(f, \change{f})$
  and, by monotonicity of $\cplus$, it follows that 
  $\fixpoint_A(f) \cplus \change{w} \leq \fixpoint_A(f) \cplus D(f, \change{f})$. But
  since $\fixpoint_A(f) \cplus D(f, \change{f})$ is the least fixpoint of $f \cplus \change{f}$,
  it must be the case that 
  $$
  \fixpoint_A(f) \cplus D(f, \change{f}) = \fixpoint_A(f) \cplus \change{w}
  $$
  thus $\fixpoint_A(f) \cplus \change{w}$ is the least fixpoint of $f \cplus \change{f}$.
\end{proof}

The requirement that $\fixpoint$ be differentiable may seem awkward, but it is
true for any complete change structure, in particular the superposition change
structure on lattices (and $L_\superpose$ on Boolean algebras).

Sadly, the explicit derivative for fixpoints is far from simple to compute. It requires a fixpoint over the
change structure partial order, which may not be possible (depending on our
change structure). However, for lattices the $L_\superpose$ change structure is
itself a lattice, so we can compute fixpoints over it.

It may not be too expensive to compute a fixpoint, since the alternative
``update strategy'' is the worst possible one: throw everything away and start
again (which will itself require a fixpoint computation).

\section{Datalog}
\label{sec:datalog}

Datalog is a well-known simple logic programming language. It is also a textbook
example of the need for incremental computation, as its semantics rely on
computing a least fixpoint by bottom-up iteration.

There is an extensive existing literature on incremental computation of Datalog
(CITES), but the aim of this section is to argue that by viewing the computation
of Datalog semantics as composed of differentiable functions we can give a
superior account.

Firstly, however, we must show that we can see the semantics of Datalog in terms
of elements that we know how to handle: Boolean algebras and fixpoints.

\subsection{Denotational semantics}

Datalog is usually given a logical semantics wherein we look for models that
satisfy the program. We will instead give a simple denotational semantics that treats a Datalog
program as denoting a family of relations.

One obstacle to this approach is giving a denotation to negations. We adopt the
closed-world assumption to solve this.

\begin{defn}[Closed-world assumption and negation]
  There exists a universal relation $\universalRel$.
  
  Negation on relations is then defined as $$\neg R \defeq \universalRel \setminus R$$
\end{defn}

This makes $\Rel$ into a Boolean algebra.

\begin{defn}[Term semantics]
  A Datalog term $T$ denotes a function from its free relation variables to
  $\Rel$, $\denote{\_} : \Term \rightarrow \Rel^n \rightarrow \Rel$
  \begin{eqnarray*}
    \denote{R_i}(\overline{X}) &\defeq& X_i \text{ where } X_i \text { is the ith free relation variable }\\
    \denote{T \wedge U}(\overline{X}) &\defeq& \denote{T}(\overline{X}) \cap  \denote{U}(\overline{X})\\
    \denote{T \vee U}(\overline{X}) &\defeq& \denote{T}(\overline{X}) \cup  \denote{U}(\overline{X})\\
    \denote{\exists x. T}(\overline{X}) &\defeq& \pi_x(\denote{T}(\overline{X}))\\
    \denote{\neg T}(\overline{X}) &\defeq& \neg \denote{T}(\overline{X})\\
    \denote{x=y}(\overline{X}) &\defeq& \Delta(x, y) \text{ where } \Delta \text{ is the diagonal relation }
  \end{eqnarray*}
\end{defn}

Since $\Rel$ is a Boolean algebra, and so is $\Rel^n$, the denotation
functions of terms are functions between Boolean algebras.

\begin{defn}[Immediate consequence operator]
  Given a program $\mathcal{P} = \overline(P)$, the immediate consequence operator $\consq: \Rel^n \rightarrow \Rel^n$ is defined as follows:
  $$\consq(\overline{R}) = \overline{\denote{P_i}(\overline{R})}$$
\end{defn}

That is, given a value for the program, we pass in all the relations
to the denotation of each predicate, to get a new product of relations.

\begin{defn}[Program semantics]
  The semantics of a program $\mathcal{P}$ is defined to be 
  $$\denote{\mathcal{P}} \defeq \lfp_{\Rel^n}(\consq)$$
  and may be calculated by repeated application of $\consq$ to $\bot$.
\end{defn}

Whether or not this program semantics exists will depend on whether the fixpoint
exists. Typically this is ensured by constraining the program such that $\consq$
is monotone. However, we will remain agnostic on this front - we are merely
interested in calculating the semantics faster if it exists.

\subsection{Differentiability of Datalog semantics}

In order to apply the machinery we have developed, we need the $\denote{_}$ to
be differentiable. However, it is a function on Boolean algebras, and we know
that the $L_\superpose$ change structure is complete, so in fact we know that
$\denote{\_}$ must be differentiable.

However, this does not mean that we have a \emph{good} derivative for
$\denote{\_}$. The derivative that we know we have for complete change structures
is quite bad:
$$\derive{f}(a, \change{a}) = f(a \cplus \change{a}) \cminus f(a)$$
Naively computed, this requires \emph{more} work than evaluating $f(a \cplus \change{a})$ directly!

However, \ref{cor:booleanCharacterization} gives us a range of derivatives to
choose from, and we can optimize within that range to find one that satisfies
our constraints.

In the case of Datalog, the change ordering on the change structure also
corresponds to the size of the derivative as a pair of relations. The minimal
derivative contains precisely the elements that are newly added or removed,
whereas the maximal derivative contains all the elements that have \emph{ever}
been added or removed. This means that \ref{cor:booleanCharacterization} allows
us to \emph{approximate} the most precise derivative while still being
guaranteed that the result is sound.\footnote{The idea of using an approximation
to the precise derivative, and a soundness condition, appears in \cite{bancilhon1986amateur}.}

There is also the question of how to compute the derivative. Fortunately, the
maximal and minimal minus operators are actually representable as pairs of terms
in our Datalog, and so we can compute the derivative via a pair of terms that
satisfy those bounds, allowing us to reuse our machinery for evaluating Datalog
terms.\footnote{Indeed, if this process is occurring in an optimizing compiler,
  the derivative terms can themselves be optimized as well. This is likely to be
  beneficial, since as we will see the initial terms may be quite complex.}

This does give us additional constraints that the derivative terms must satisfy:
for example, we may need to preserve safety or range restriction in order to be
able to evaluate them; and we may wish to pick terms that will be easy or cheap
for our evaluation engine to evaluate, even if the results are larger.

The upshot of these considerations is that the optimal choice of derivatives is likely
to be quite dependent on the precise variant of Datalog being evaluated, and the
specifics of the evaluation engine. Here is one possibility.\footnote{These are
  the rules actually in use at Semmle. We arrived at them by starting with the
  minimal derivative and then simplifying and weakening it while preserving the
  bound given by the maximal derivative.}

\newcommand{\bothdiff}{\diamond}
\begin{thm}[Concrete Datalog term derivatives]
\label{thm:concreteDatalog}
  We give two mutually recursive definitions,
  $\updiff: \Term \rightarrow \changes{\Rel^n} \rightarrow \changes{Rel}$ and
  $\downdiff: \Term \rightarrow \changes{\Rel^n} \rightarrow \changes{Rel}$, such
  that $\updiff \times \downdiff$ is a derivative for Datalog term semantics.
  
  Let $$\diamond X \defeq X \twist (\updiff X, \downdiff X)$$ in the following.

  TODO: make the references to the argument changes more obvious
  
  \begin{eqnarray*}
  \updiff\bot & \defeq & \bot\\
  \updiff\top & \defeq & \bot\\
  \updiff R & \defeq & \updiff R\\
  \updiff(T\vee U) & \defeq & \updiff T \vee \updiff U\\
  \updiff(T\wedge U) & \defeq & (\updiff T\wedge \bothdiff U) 
                           \vee 
                           (\updiff U \wedge \bothdiff T)\\
  \updiff(\neg T) & \defeq & \downdiff T\\
  \updiff(\exists x.T) & \defeq & \exists x.\updiff T
  \end{eqnarray*}

  \begin{eqnarray*}
  \downdiff\bot & \defeq & \bot\\
  \downdiff\top & \defeq & \bot\\
  \downdiff R & \defeq & \downdiff R\\
  \downdiff(T\vee U) & \defeq & (\downdiff T \wedge \neg \bothdiff U) 
                           \vee 
                           (\downdiff U \wedge \neg \bothdiff T)\\
  \downdiff(T\wedge U) & \defeq & (\downdiff T\wedge U) \vee (T \wedge \downdiff U)\\
  \downdiff(\neg T) & \defeq & \updiff T\\
  \downdiff(\exists x.T) & \defeq & \exists x.\downdiff T \wedge \neg \exists x.\bothdiff T
  \end{eqnarray*}
\end{thm}
\begin{proof}
  Long, tedious structural induction. Maybe in an appendix?
\end{proof}

There is a symmetry between the cases for $\wedge$ and $\vee$ between $\updiff$
and $\downdiff$, but the cases for $\exists$ look quite different. 
This is because we have chosen a dialect of Datalog without a primitive universal quantifier.
If we did have one, its cases would be dual to those for $\exists$, namely:
\begin{eqnarray*}
\updiff(\forall x.T) & = & \exists x. \updiff T \wedge \forall x. \bothdiff T\\
\downdiff(\forall x.T) & = & \forall x. \downdiff T
\end{eqnarray*}

We can easily extend a derivative for the term semantics to a derivative for $\consq$.

\begin{corollary}
\label{corollary:consqDiff}
  $\consq$ is differentiable.
\end{corollary}

\subsection{Differential evaluation of Datalog}

Putting this together, we get two results.

\begin{thm}[Differential evaluation of Datalog semantics]
\label{thm:diffEval}
  Datalog program semantics can be evaluated incrementally.
\end{thm}
\begin{proof}
  Corollary of \ref{corollary:diffFP} and \ref{corollary:consqDiff}.
\end{proof}

This is a generalization of semi-naive evaluation.

\begin{thm}[Differential update of Datalog semantics]
\label{thm:diffUpdate}
  Datalog program semantics can be incrementally updated with changes to non-recursive relations.
\end{thm}
\begin{proof}
  Corollary of \ref{thm:fixpointDiff} and \ref{corollary:consqDiff}.
\end{proof}

This is a generalization of the view-update problem, including recursive relations.

\subsection{Extensions to Datalog}

Our formulation of Datalog term semantics and differential evaluation is quite
generic and composable, so it is relatively easy to extend the language with new
term constructs.

A new term construct must have:
\begin{itemize}
  \item An interpretation as a function on its free relation variables.
  \item An implementation of $\updiff$ and $\downdiff$.
\end{itemize}

These are very easy conditions - the former is needed for the construct to even
make sense, and the latter can always be satisfied by using the maximal or
minimal derivative. Although they are very bad derivatives, having them
available as options is very helpful. It means that even if we have a term
construct which we cannot find a good derivative for, we only lose incremental
evaluation for those subterms, and not for anything else.

Note that this does not say anything about monotonicity. New term constructs may
be required to be monotonic if they are to participate in recursion, but we are
interested in derivatives even for non-monotonic terms because they allow us to
use \ref{thm:diffUpdate}.

This is important in practice for Semmle's variant of Datalog, which includes
aggregation and other primitives with interesting derivatives.

\section{Related work}

\subsection{Change structures}

The seminal paper in this area is Cai et al.~\cite{cai2014changes}. We use the notions
defined in that excellent paper heavily, but we deviate in two regards: the use of
dependent types, and the nature of function changes.

These two issues are linked, because part of the reason that Cai et al. need
dependently typed changes is in order to handle their notion of function
changes.

Our notion of function changes is different to Cai et al.'s because they require the
function changes to behave like derivatives. To our eyes, this confuses two ways
in which functions can change: the derivative gives the change in the function
at a point, as the point moves; whereas a change in the function itself is a
global change to the function at all points.

Furthermore, our notion of function changes provides a more natural structure,
in that it allows us to make the category of change structures into a cartesian
closed category.

Our function changes do not need to be dependently typed, so we do not need a
dependently typed formulation for the cases we have been considering. However,
if we want to extend this formulation to differential geometry we will need
dependently typed changes, since the type of the tangent space is
dependent on the point at which the tangents are taken.

In fact, a slightly different dependently-typed formulation seems promising, since it would allow us
to draw out a 2-categorical interpretation of change structures. For example, if
we consider our base set as a category, with changes $\Delta_a^b$ between $a$
and $b$ as morphisms between $a$ and $b$, then a function is
differentiable iff it is a functor on that category (i.e. provides a change in $\Delta_{f(a)}^{f(b)}$).

While this is a promising direction, it wasn't necessary for the material in
this paper, and would have complicated the exposition significantly, so we opted
to leave it for future work.

S-acts and their categorical structure have received a fair amount of attention
over the years (Kilp, Knauer, and Mikhalev~\cite{kilp2000monoids} is a good overview). However, our work
is largely distinct from this, since we are interested in acts over different
monoids/semigroups, which necessitates the presence of derivatives.

Arntzenius~\cite{arntz2017fixpoints} gives a derivative operator for fixpoints. However,
it uses the definition of function changes from Cai et al., whereas
we have a different notion of function changes, so the result is unfortunately
inapplicable. In addition, we prove our result for all functions, not just
monotone functions.

\subsection{Datalog}

The earliest explication of semi-naive evaluation as a derivative process
appears in Bancilhon~\cite{bancilhon1986naive}. The idea of using an approximate derivative
and the requisite soundness condition appears as a throwaway comment in
Bancilhon and Ramakrishnan~\cite{bancilhon1986amateur}, although as far as we know nobody has since
developed that approach. 

There is existing literature on incremental updates to relational algebra
expressions. In particular Griffin et al.~\cite{griffin1997improved} following
Qian and Wiederhold~\cite{qian1991incremental} shows the essential insight that it is necessary to
track both an ``upwards'' and a ``downwards'' difference, and produces a set of
rules that look quite similar to those we derive in \ref{thm:concreteDatalog}.

Where our presentation improves over Griffin et al. is mainly in
the genericity of the presentation. Our machinery works for a wider variety of
algebraic structures, and it is clear how the parts of the proof work together
to produce the result. In addition, it is easy to see how to extend the proofs
to cover additional language constructs.

Griffin et al. are interested in \emph{minimal} changes. These correspond to
minimal derivatives in the sense of \ref{cor:booleanCharacterization}. However,
while minimality (or proximity to minimality) is desirable (since it decreases
the size of the derivatives as relations), it is important to be able to trade
it off against other considerations. For example, at
Semmle we use the derivatives given in \ref{thm:concreteDatalog}, which are not minimal.

There are some inessential points of difference as well: we work on Datalog,
rather than relational algebra; and we use set semantics rather than bag
semantics. This is largely a matter of convenience: Datalog is an easier
language to work with, and set semantics allows a much wider range of valid
simplifications. However, all the same machinery applies to relational algebra
with bag semantics, it is simply necessary to produce a valid version of \ref{thm:concreteDatalog}.

We also solve the problem of updating \emph{recursive} expressions. As far as we
know, this is unsolved in general. Most of the attempts to solve it have
focussed on Datalog rather than relational algebra, since Datalog is designed to
make heavy use of recursion. 

Gupta et al.~\cite{gupta1995maintenance} has a good summary of the approaches so far.

WRITE MORE HERE

Datafun (Arntzenius~\cite{arntz2016datafun}) is a functional programming language that embeds
Datalog, allowing significant improvements in genericity, such as the use of
higher-order functions. Since we have directly defined a change structure and
derivative operator for Datalog, our work could be used as a ``plugin'' in the sense
of Cai et al., allowing Datafun to compute its internal fixpoints
incrementally, but also allowing Datafun expressions to be fully incrementally updated.

\bibliography{paper}

\end{document}
