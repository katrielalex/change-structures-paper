% Toggle comments for preamble and topmatter to typeset in ACM style

%\input{preamble-standard}
\input{preamble-acm}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{stmaryrd}
\usepackage{biblatex}
\usepackage{hyperref}
\usepackage{cleveref}

\input{notation}

\addbibresource{paper.bib}

\begin{document}

%\input{topmatter-standard}
\input{topmatter-acm}

\begin{abstract}
  Incremental computation has recently been studied using the concept of a
  \textit{derivative} of a program. This general notion allows updating programs
  based on changes in their inputs

  We generalise the notion of derivative, and study its categorical
  properties. We develop the resulting machinery on a variety of common structures
  in computer science, including partial orders, lattices, and Boolean algebras.

  This culminates in a generic and compositional account of incremental evaluation of Datalog, as
  well as incremental update of (recursive) Datalog programs.
\end{abstract}

\title{Something about change structures}

\maketitle

\section{Introduction}

The paper of \textcite{cai2014changes} defines compelling notions
of change structure and derivative, and use them to provide a composable framework for
evaluating incremental lambda calculus programs provided you have a ``plugin'' for the
primitive operations of your program.

This provides an excellent foundation for using change structures, but there are
many questions remaining about change structures themselves. What are their
algebraic properties? Can we define change structures on more varied kinds of
structures? What are the interactions with orders on the base set?

There are two major motivational targets for this exercise. The first is
lattices. Incremental computation is especially attractive for database or logic
programming languages (Datalog, SQL, etc.), where it can produce drastically
smaller results than naive evaluation. We therefore want to understand
incremental computation in its most general form on lattices, so that we have
maximum flexibility when bringing it to bear on a particular application. For
applications to Datalog in particular, see (cite forthcoming paper, or perhaps
my arxiv preprint).

The calculus of differential geometry is significantly more abstract than real
calculus. The connection to change structures is still tenuous, but there are
telling similarities, and it is our eventual goal to find a sufficiently general
notion of derivative that will encompass differential geometry as well.

The major contributions of this paper are:
\begin{itemize}
  \item A generalised definition of change structures and derivatives
    (\cref{sec:change-structures}).
  \item A description of the categorical properties of change structures, including
    several useful direct constructions (\cref{sec:category}).
  \item A description of the properties of change structures over several
    additional kinds of structures, including theorems that characterize the range of
    possible derivatives available, and conditions under which compound change
    structures may admit derivatives (\cref{sec:moreStructures}).
  \item Two results relating to fixpoint calculations: incremental computation
    of fixpoints, and incremental updates of fixpoint expressions (\cref{sec:fixpoints}).
  \item Applications of the above machinery to incremental computation and
    update of Datalog programs, with the ability to handle arbitrary additional
    language constructs, including negation and aggregation (\cref{sec:datalog}).
\end{itemize}

\section{Change structures}
\label{sec:change-structures}

MARIO: Should we pick a different name to distinguish them from Cai-style change structures?

MPJ: Not sure. I do really like ``change structure'', and I think they're
similar, but perhaps we should use a different name. Forward algebra? Change action?

MARIO: I've been calling them ``forward algebras'', but the specific name is not particularly
important. I would definitely like to have a different name, especially since we often contrast the two.

MPJ: I propose ``change action'', since we're emphasising the monoid action
connection. Really, I think they're just semigroup/monoid actions with a more
general notion of morphism.

MARIO: Can we use something that references the monoid? Like `monoid system' or
`monoid algebra' or `monoid thingamajig'? So we can talk about group systems, vector
space systems and similar.
I'd just call them monoid actions, but that usually refers to the function itself, not
the whole thing.

MPJ: I do like the idea of just calling them monoid actions, but I think having
a different name is useful because e.g. it gives us something to call our
category, which has some additional assumptions on morphisms that aren't
standard for monoid actions.

\begin{defn}[Change structures]
  A \textit{change structure} is defined as:

  $$\mathcal{A} \defeq \cstruct{A}{\changes{A}}{\cplus}$$

  where $A$ is a set, $(\changes{A}, \splus, \mzero)$ is a monoid, and $\cplus$ gives a monoid action on $A$.

  We will call $A$ the base set, and $\changes{A}$ the change set of the change structure.
\end{defn}

Elements in the change set represent changes that can be made to elements in the
base set, with the monoid action being the operation that ``applies'' the
change. The requirement that the change set be a monoid is convenient but in
fact inessential: given any set with an action on the base set, we can take the
free monoid over the action set to obtain a monoid action.

The fact that the change set is a monoid action reveals the heart of the
change structure definition: the change set is a representation of a
\textit{transformation monoid} over the base set, with the monoid
operation simply being composition.

The primary motivation for change structures is that they let us define
\texit{derivatives} for functions.

\begin{defn}[Derivatives]
  A \textit{derivative} of a function $f: A_\cplus \rightarrow B_\cpluss$ is a function $\derive{f}: A \times \changes{A} \rightarrow
  \changes{B}$ such that
  $$f(a \cplus \change{a}) = f(a) \cpluss \derive{f}(a, \change{a})$$

  A function which has a derivative is called \textit{differentiable}.
\end{defn}

Derivatives need not be unique, in general, so we will speak of ``a''
derivative. 

Derivatives capture the essence of incremental computation: given the value of a
function at a point, and a change to that point, they tell you how to compute
the new value of the function.

The choice of the name ``derivative'' is also not a coincidence. While these
derivatives do not look quite like derivatives in real analysis, they \emph{do}
bear a strong resemblance to derivatives in other areas (such as synthetic differential geometry), and
they satisfy the standard chain rule.

\begin{thm}[The Chain Rule]
  Let $f: A_\cplus \rightarrow B_\cpluss$, $g: B_\cpluss \rightarrow C_\cplusss$ be differentiable functions. Then $g \circ f$ is also
  differentiable, with derivative given by
   $$\derive{(g \circ f)}(x, \change{x}) = \derive{g}\left(f(x), \derive{f}(x, \change{x})\right)$$
   or, in curried form
   $$\derive{(g \circ f)}(x)(\change{x}) = \derive{g}(f(x)) \circ \derive{f}(x)$$
\end{thm}
\begin{proof}
  By equivalence:
  \begin{itemize}
    \item[ ]$(g \circ f)(x) \cplusss \derive{g}\left(f(x), \derive{f}(x,\change{x})\right)$
    \item[=]$(g(f(x)) \cplusss \derive{g}\left(f(x), \derive{f}(x,\change{x})\right)$
    \item[=]$g\left(f(x) \cpluss \derive{f}(x, \change{x}) \right)$
    \item[=]$g\left(f(x \cplus \change{x})\right)$
    \item[=]$(g \circ f)(x \cplus \change{x})$
  \end{itemize}
  Therefore $\derive{g}\left(f(x), \derive{f}(x, \change{x})\right)$ is a
  derivative for $(g \circ f)$.
\end{proof}

Here are some recurring examples of changes structures:
\begin{itemize}
  \item $A_\discrete \defeq \cstruct{A}{\emptyset}{\emptyset}$, the discrete change structure on any base set.
  \item $A_\Rightarrow \defeq \cstruct{A}{A\Rightarrow A}{\ev}$, where $A
    \Rightarrow A$ denotes the exponential object and $\ev$ is the evaluation map.
  \item $A_\leq \defeq \cstruct{A}{\leq}{ev_\leq}$, where $\leq$ is some
    transitive relation on $A$ and $ev_\leq$ denotes
    conditional application, i.e. $ev_\leq(a, (b, c))$ is equal to $c$ if $a = b$, and $a$ otherwise. Composition of changes is obtained
    by transitivity of $\leq$.
  \item $\mathbb{Z}_1 \defeq \cstruct{\mathbb{Z}}{\mathbb{N}}{+}$
  \item $\mathbb{Z}_2 \defeq \cstruct{\mathbb{Z}}{\mathbb{N}}{-}$
  \item $\mathbb{Z}_3 \defeq \cstruct{\mathbb{Z}}{\mathbb{Z}}{+}$
  \item $F_2 \defeq$ integers modulo 2 with addition.
\end{itemize}

Indeed, any monoid $(A, \splus)$ can be seen as a change structure $\cstruct{A}{A}{\splus}$. In particular,
for any change structure $\cstruct{A}{\changes{A}}{\cplus}$,
$\cstruct{\changes{A}}{\changes{A}}{\splus}$ is also a change structure. Many practical change structures
can be constructed in this way:
\begin{itemize}
  \item $[A]$, the type of lists (or streams) of elements of type $A$, is a monoid under
  concatenation. Hence it defines a change structure $\cstruct{[A]}{[A]}{\doubleplus}$.
  \item $\{A\}$, the type of sets, is a monoid under either set union or intersection,
  and thus it defines change structures $\cstruct{\{A\}}{\{A\}}{\cup},\cstruct{\{A\}}{\{A\}}{\cap}$
\end{itemize}

Many other notions in computer science can be naturally understood in terms of change structures, e.g. databases
and database updates, files and \textit{diff}s, Git repositories and commits, even video compression
algorithms that encode a frame as a series of changes to the previous frame.

\subsection{Minus operators and completeness}

\textcite{cai2014changes} include a ``minus operator'' in their definition of change structures. 

\begin{defn}[Minus operator]
  A \textit{minus operator} is a function $\cminus: A \times A \rightarrow \changes{A}$ such that $a \cplus (b \cminus a) = b$.
\end{defn}

We have omitted minus operators from our definition because
there are many interesting change structures that do not have them (for example,
$\mathbb{Z}_3$ does, but $\mathbb{Z}_1$ does not). Indeed, requiring change structures to provide
a minus operator is overly restrictive, and would often force one to adopt unwieldy representations
for change sets (for example, the change structures described before on sets and lists are clearly
useful for incremental computation on streams, yet they do not admit minus operators - instead, one would
be forced to work with e.g. multisets admitting negative arities, as \textcite{cai2014changes} do).

\begin{defn}[Completeness]
  A change structure is \textit{complete} if for any $a, b \in A$, there is
  a change $\change{a} \in \changes{A}$ such that $a \cplus \change{a} = b$.
\end{defn}

\begin{prop}[Completeness equivalences]
  Let $A$ be a change structure. Then the following are equivalent:
  \begin{itemize}
    \item $A$ is complete.
    \item The monoid action is transitive.
    \item There is a minus operator on $A$.
    \item Any function from any change structure into $A$ is differentiable.
  \end{itemize}
\end{prop}

This last property is of the utmost importance, since we are often concerned with the differentiability
of functions.

\begin{defn}[Minus derivative]
  Given a minus operator $\cminus$, we have a derivative for any function $f$,
  defined as
  $$\derive{f}_\cminus(a, \change{a}) \defeq f(a \cplus \change{a}) \cminus f(a)$$
\end{defn}

\subsection{Thin change structures}

As in \textcite{cai2014changes}, multiple changes may represent the difference
between two elements. This is true for many change structures, but the change
structures for which there \emph{is} only one such change are particularly
well-behaved, so it's worth naming them.

\begin{defn}[Thin change structures]
  A change structure is \textit{thin} if whenever $a \cplus \change{a}
  = a \cplus \change{b}$ for some $a \in A$, it is the case that $\change{a} = \change{b}$.
\end{defn}

Many change structures are not thin, for example $F_2$.

\begin{prop}[Thin equivalences]
  Let $A$ be a change structure. Then the following are equivalent.
  \begin{itemize}
    \item $A$ is thin.
    \item The monoid action is free.
    \item If $A$ has a minus operator $\cminus$, then $(a \cplus \change{a})
      \cminus a = \change{a}$.
  \end{itemize}
\end{prop}

Thinness gives us uniqueness of derivatives:

\begin{prop}
  Let $B$ be a thin change structure, and $f: A \rightarrow B$. Then $f$ has at
  most one derivative.

  Conversely, if $\textrm{id}: B \rightarrow B$ has exactly one derivative, then
  $B$ is thin.
\end{prop}

\section{The category of change structures}
\label{sec:category}

\begin{defn}[Category of change structures]
  We define the category $\cat{CStruct}$ of change structures. The objects are
  change structures and the morphisms are differentiable functions. We denote
  the set of differentiable functions between $A$ and $B$ as $A \difffunc B$.
\end{defn}

\subsection{Equivalence with PreOrd}

There is a natural preorder on the base set of a change structure, given by reachability
 under the action:
\begin{defn}[Reachability order]
  $a \reachOrder b$ iff there is a $\change{a} \in \changes{A}$ such that $a \cplus
  \change{a} = b$.
\end{defn}

The reachability order of a complete change structure on $A$ is precisely the trivial relation
$A \times A$. Conversely, any such change structure is necessarily complete. The correspondence
between a change structure and its corresponding reachability preorder gives rise to
a faithful functor $Reach : \cat{CStruct} \rightarrow \cat{PreOrd}$ that acts as the identity
on morphisms.

\begin{prop}
  A function is differentiable iff it is monotonic with respect to the
  reachability order. Equivalently, the functor $Reach$ is full.
\end{prop}

\begin{corollary}
  Two change structures are isomorphic iff their posets under the reachability
  order are isomorphic.
\end{corollary}

\begin{corollary}
  Any function from a discrete change structure or into a complete change
  structure is differentiable.
\end{corollary}

Conversely, any preorder $\leq$ on some set $A$ induces the corresponding change structure
$A_\leq$. This gives rise to a (full and faithful) functor $\{\_\}_\leq : \cat{PreOrd} \rightarrow \cat{CStruct}$

\begin{thm}[Equivalence to $\cat{PreOrd}$]
  The functor $Reach$ from $\cat{CStruct}$ to $\cat{PreOrd}$ together with the 
  functor $\{\_\}_\leq$ in the opposite direction form an equivalence of categories.
\end{thm}
\begin{proof}
  On one hand, if $U$ is a preorder, it's trivial to check that $Reach (\{U\}_\leq) = U$.

  On the other hand, we need to find a natural isomorphism between $\{\_\}_\leq \circ Reach$
  and the identity functor. First, we note that the base set for the change structure $\{Reach(A)\}$ is
  the same as the base set for $A$. We claim that the desired natural isomorphism is given by the
  the identity on the base sets. It remains to prove that the identities
  $id_A : A \rightarrow \{Reach(A)\}$ and $id_{\reachOrder} : \{Reach(A)\} \rightarrow A$
  is indeed differentiable in both directions.

  On one direction, a derivative is given by
  $$
    \derive{id}_A(a, \change{a}) \defeq (a, a \cplus \change{a})
  $$
  Conversely, let $(a, b) \in \reachOrder$. By definition of $\reachOrder$, there is some
  $\change{}_{(a, b)} \in \changes{A}$ satisfying $a \cplus \change{}_{(a,b)} = b$. Hence we set the
  derivative to be
  $$
    \derive{id}_{\reachOrder}(a, (a, b)) \defeq \change{}_{(a, b)}
  $$
  MARIO: the axiom of choice strikes again...
\end{proof}

Since $\cat{PreOrd}$ is a reflective subcategory of $\cat{Set}$, this gives us a proof
of the existence of limits and colimits in $\cat{CStruct}$.

MPJ: and exponentials? or do we actually need the constructive proof of that?

\begin{corollary}
  The category $\cat{CStruct}$ has all limits, colimits and exponential objects. 
\end{corollary}

\subsection{Explicit constructions}

Having shown that $\cat{CStruct}$ is equivalent to $\cat{PreOrd}$ it may seem
redundant to give explicit constructions of some of the useful categorical
notions. However, we can give constructions that are much nicer than the ones
gained by going via $\cat{PreOrd}$, which is important for using them in
practical computation.

\begin{prop}[Products]
  Let $A = \cstruct{A}{\changes{A}}{\cplus}$ and $B =
  \cstruct{B}{\changes{B}}{\cpluss}$ be change structures.

  Then $A \times B \defeq \cstruct{A \times B}{\changes{A} \times
  \changes{B}}{\cplus \times \cpluss}$ is their categorical product.
\end{prop}
\begin{proof}
  Let $Y$ be a change structure, and $f_1: Y \rightarrow A$, $f_2: Y
  \rightarrow B$ be morphisms.

  Then the product morphism in $\cat{Set}$, $\pair{f_1}{f_2}$ is the product
  morphism in $\cat{CStruct}$. It can easily be
  shown that $\pair{\derive{f_1}}{\derive{f_2}}$ is a derivative of $\pair{f_1}{f_2}$,
  hence $\pair{f_1}{f_2}$ is a morphism in $\cat{SAct}$.

  Commutativity and uniqueness follow from the corresponding properties of the
  product in the $\cat{Set}$.
\end{proof}

\begin{prop}[Coproducts]
  Let $A = \cstruct{A}{\changes{A}}{\cplus}$ and $B =
  \cstruct{B}{\changes{B}}{\cpluss}$ be change structures.

  Then $A + B \defeq \cstruct{A + B}{\changes{A} \times
  \changes{B}}{\cplusvee}$ is their categorical coproduct, with $\cplusvee$ defined as:
  $$i_1 a \cplusvee (\change{a}, \change{b}) \defeq \iota_1 (a \cplus \change{a})$$
  $$i_2 b \cplusvee (\change{a}, \change{b}) \defeq \iota_2 (b \cplus \change{b})$$
\end{prop}
\begin{proof}
  Let $Y$ be a change structure, and $f_1 : A \rightarrow Y$, $f_2 : B
  \rightarrow Y$ be differentiable maps.

  As before, it suffices to prove that the universal map $[f_1, f_2]$ in $\cat{Set}$ is a differentiable
  map from $\cstruct{A + B}{\changes{A} \times \changes{B}}{\cplusvee}$ into $Y$. It's easy to see 
  that the following morphism is a derivative:
  $$[f_1, f_2]' (i_1 a, (\change{a}, \change{b})) \defeq f_1'(a, \change{a})$$
  $$[f_1, f_2]''' (i_2 b, (\change{a}, \change{b})) \defeq f_2'(b, \change{b})$$
\end{proof}

MPJ: do we have a proof that Cai's formulation doesn't give a CCC? That would be
nice.

MPJ: actually, I think they *do* form a CCC: you just use the function change
argument to get the derivative of of ev, and then it works by their
incrementalization lemma. You can use the same derivative of $\curry{f}$. Bah.

\begin{prop}[Exponentials]
\label{prop:exponentials}
  Let $A_\cplus$ and $B_\cpluss$ be change structures

  Then $\cstruct{A \difffunc B}{A
    \rightarrow \changes{B}}{\lambda d. \lambda f. \lambda a. f(a) \cpluss
    d(a)}$ is a change structure on $A \difffunc B$ and the exponential object.

  The monoid structure on $A \rightarrow \changes{B}$ is the monoid
  structure on $\changes{B}$ lifted pointwise, so we will typically reuse the
  change operator for $B$ for $A \rightarrow \changes{B}$.
\end{prop}
\begin{proof}
  Since we are working over $\cat{Set}$, we can use the definitions of $\ev$ and
  $\curry{f}$ from $\cat{Set}$:

  $$\ev(f, a) \defeq f(a)$$
  $$\curry{f}(a) \defeq \lambda b. f((a, b))$$
  
  We merely need to show that they are differentiable: differentiability of $\curry{f}
  \times id$ follows from products; commutativity and
  uniqueness follow from the corresponding properties of the exponential object
  on $\cat{Set}$.

  Let 
  $$\derive{\ev}((f, a), (\change{f}, \change{a})) \defeq \derive{f}(a, \change{a}) \splus \change{f}(a \cplus \change{a})$$

  Then
  \begin{itemize}
    \item[ ]$\ev((f, a) \cplus (\change{f}, \change{a}))$
    \item[=]$\ev((f \cplus \change{f}, a \cplus \change{a}))$
    \item[=]\{ definition of $\ev$ \}\\
      $(f \cplus \change{f})(a \cplus \change{a})$
    \item[=]\{ definition of $\changes{(A \rightarrow B)}$ \}\\
      $f(a \cplus \change{a}) \cplus \change{f}(a \cplus \change{a})$
    \item[=]\{ $f$ is differentiable \}\\
      $f(a) \cplus \derive{f}(a, \change{a}) \cplus \change{f}(a \cplus \change{a})$
    \item[=]\{ monoid action property \}\\
      $f(a) \cplus \left[ \derive{f}(a, \change{a}) \splus \change{f}(a \cplus \change{a})\right]$
    \item[=]\{ definition of $\derive{\ev}$ \}\\
      $\ev(f, a) \cplus \derive{\ev}((f, a), (\change{f}, \change{a}))$
  \end{itemize}
  Therefore $\derive{\ev}$ is a derivative for $\ev$, so $\ev$ is differentiable.
  
  Let
  $$\derive{\curry{f}}(a, \change{a}) \defeq \lambda b. \derive{f}((a, b),
  (\change{a}, \mzero))$$
  
  Then
  \begin{itemize}
    \item[ ]$\curry{f}(a \cplus \change{a})$
    \item[=]\{ definition of $\curry{f}$ \}\\
      $\lambda b. f((a \cplus \change{a}, b))$
    \item[=] $\lambda b. f((a, b) \cplus (\change{a}, \mzero))$
    \item[=]\{ since $f$ is differentiable \}\\
      $\lambda b. f((a, b)) \cplus \derive{f}((a, b), (\change{a}, \mzero))$
    \item[=]\{ definition of $\changes{(A \rightarrow B)}$ \}\\
      $(\lambda b. f((a, b))) \cplus (\lambda b. \derive{f}((a, b), (\change{a}, \mzero)))$
    \item[=]\{ definition of $\derive{\curry{f}}$ \}\\
      $\curry{f}(a) \cplus \derive{\curry{f}}(a, \change{a})$
  \end{itemize}

  Therefore $\derive{\curry{f}}$ is a derivative for $\curry{f}$, so $\curry{f}$
  is differentiable.
\end{proof}

MARIO: nice proof, I'm a bit worried about the $f'$, though, since there may be more than one,
and I feel like we need the axiom of choice to ``pick'' a $f'$ for every $f$.

MPJ: there's only one $f$ here - we only need AoC if we're doing *arbitrary*
choices for *infinitely* many $f$s.

MARIO: I'm referring to the proof of exponentials - in order to define $\derive{ev}$ you
need it to pick a $\derive{f}$ for each $f$.

MPJ: oh I see - yes, perhaps. I think that's likely to be a problem in a number
of places. (I don't particularly mind the AoC, especially since we don't care
about constructing this thing)

Derivatives of the evaluation map correspond to incremental evaluation of functions:

\begin{prop}[Incrementalization]
\label{prop:incrementalization}
  Let $f: A \rightarrow B$, $a \in A$, $\change{a} \in
  \changes{A}$, $\change{f} \in \changes{(A \rightarrow B)}$, and let
  $\derive{\ev}$ be a derivative of the evaluation map.

  Then 
  $$(f \cplus \change{f})(a \cplus \change{a}) = f(a) \cplus \derive{\ev}((f, a), (\change{f}, \change{a}))$$
\end{prop}

Conveniently, our proof of \cref{prop:exponentials} gives us actual derivatives for the evaluation map:

\begin{prop}[Derivatives of the evaluation map]
\label{prop:evDerivatives}
  Let $f: A \rightarrow B$, $a \in A$, $\change{a} \in
  \changes{A}$, $\change{f} \in \changes{(A \rightarrow B)}$.

  Then if $f$ is differentiable
  $$\derive{\ev}_1 \defeq \derive{f}(a, \change{a}) \splus \change{f}(a \cplus \change{a})$$

  is a derivative for the evaluation map.
  
  Or, if $f \cplus \change{f}$ is differentiable
  $$\derive{\ev}_2 \defeq \change{f}(a) \splus \derive{(f \cplus \change{f})}(a, \change{a})$$

  is a derivative for the evaluation map.
\end{prop}
\begin{proof}
  The first is shown in the proof of \cref{prop:exponentials}, the second is
  similar but begins by taking the derivative of $f \cplus \change{f}$.
\end{proof}

\subsection{The category of thin change structures}

\begin{prop}
  The product $A \times B$ of change structures $A$ and $B$ is thin if and only if both $A$ and $B$ are.
  Furthermore, whenever $A$ and $B$ are complete, then so is $A \times B$.

  The exponential object $A \Rightarrow B$ of change structures $A$ and $B$ is thin if $B$ is. 
  Furthermore, whenever $B$ is complete, then so is $A \Rightarrow B$.
\end{prop}

Thus, the category of thin change structures is Cartesian closed as well.

\subsection{Ordering change structures}

We can put an order on the change structures for a given base set as follows:

\begin{defn}[Change structure ordering]
  $A_\cplus \fineOrder A_\cpluss$ iff $\textrm{id}: A_\cplus \rightarrow A_\cpluss$ is differentiable.
\end{defn}

Transitivity of the order follows from the chain rule, and reflexivity is trivial.

This ordering is useful because it gives us a natural sense of the ``fineness''
of a change structure, much like the corresponding version in topology.

\begin{prop}
  If $f: A_\cplus \rightarrow B_\cpluss$ is differentiable, then
  \begin{itemize}
    \item if $A_\cplusss \fineOrder A_\cplus$ then $f: A_\cplusss \rightarrow
      B_\cpluss$ is differentiable.
    \item if $B_\cplus \fineOrder B_\cplusss$ then $f: A_\cplus \rightarrow
      B_\cplusss$ is differentiable.
  \end{itemize}
\end{prop}

That is, functions remain differentiable if the source change structure becomes
coarser, or the target change structure becomes finer (again, mirroring topology).

Furthermore, $\fineOrder$ also gives a fineness ordering on the reachability orders.

\begin{prop}
  If $a \reachOrder b$ in $A_\cplus$ and $A_\cplus \fineOrder A_\cpluss$, then $a \reachOrder b$ in $A_\cpluss$.
\end{prop}

\subsection{Superpositions}

As well as combining change structures entire, we can combine two different
change structures on the same underlying set.

\begin{defn}[Superposition]
  Let $A_\cplus = \cstruct{A}{\changes{A}_\cplus}{\cplus}$ and $A_\cpluss =
  \cstruct{A}{\changes{A}_\cpluss}{\cpluss}$ be change structures.

  Then the \textit{superposition} of $A_\cplus$ and $A_\cpluss$ is defined as:
  $$A_\cplus \superpose A_\cpluss \defeq \cstruct{A}{
    (\changes{A}_\cplus \times \changes{A}_\cpluss)^\ast}{(\cplus + \cpluss)^\ast}$$
  where $X^\ast$ denotes the set of finite sequences of elements $X$.
\end{defn}

Here we have used the ``trick'' mentioned earlier to ensure that our change
set has a monoid structure: $\cstruct{A}{(\changes{A}_\cplus +
  \changes{A}_\cpluss)}{\star}$ does not have a monoid structure, so we take
the free extension to finite sequences.

In some cases we may be able to find a more compact representation of the
superposition change structure, for example $\mathbb{Z}_1 \superpose \mathbb{Z}_2$ is 
isomorphic to $\mathbb{Z}_3$. The following result shows one way of finding such representations which
we have found helpful in practice:

\begin{prop}
  Let $\changes{A}_\cplus, \cplus$ and $\changes{A}_\cpluss, \cpluss$ be two different change structures on
  a set $A$. Suppose there exists a function
  $\wr : \changes{A}_\cplus \times \changes{A}_\cpluss \rightarrow \changes{A}_\cplus$ satisfying for
  all $a \in A$:
  $$
    a \cplus \change{a} \cpluss \change{b} \cplus \change{c}
    = a \cplus (\change{a} + \wr(\change{c},\change{b})) \cpluss \change{b}
  $$

  Then $\cstruct{A}{\changes{A}_\cplus \times \changes{A}_\cpluss}{\cplussym{\wedge}}$
  is a change structurewith monoidal structure given by:
  $$
    (\change{a}, \change{b}) + (\change{c}, \change{d}) \defeq 
    (\change{a} + \wr(\change{c}, \change{b}), \change{b} + \change{d})
  $$
  and action $\cplussym{\wedge}$ defined as:
  $$
    a \cplussym{\wedge} (\change{a}, \change{b}) \defeq a \cplus \change{a} \cpluss \change{b}
  $$
\end{prop}

Superposition is a useful construction, because it gives us the coarsest
change structure finer than its components.

\begin{corollary}
  $A_\cplus \superpose A_\cpluss$ is the least upper bound of $A_\cplus$ and $A_\cpluss$ with respect to $\fineOrder$.
\end{corollary}
\begin{proof}
  The derivative of $id_1: A_\cplus \rightarrow (A_\cplus \superpose
  A_\cpluss)$ is given by $\derive{id_1}(a, \change{a}) \defeq 
  i_1(\change{a})$, and similarly for $id_2$ on the other side. Hence $A_\cplus \superpose A_\cpluss$ 
  is an upper bound of $A_\cplus$ and $A_\cpluss$.

  Let $A_\cplusss$ be greater than $A_\cplus$ and $A_\cpluss$.
  Then a
  derivative of $id: (A_\cplus \superpose A_\cpluss) \rightarrow A_\cplusss$ is
  given by :
  $$
  \derive{id}(a, (\change{a_i}, \change{b_i})) \defeq 
    \sum_i (\derive{id}(a, \change{a_i}) + \derive{id}(a, \change{b_i}))
  $$
\end{proof}

This gives us a join-semilattice for change structures on a given set.

\begin{thm}[Change structure semilattice]
  Change structures on a base set $A$ form a bounded join-semilattice 
  ordered by $\fineOrder$, with the least element given by
  $A_\discrete$, and the join operation given by $\superpose$.
\end{thm}

All functions into a complete change structure are differentiable, so any
complete change structures on $A$ will be maximal elements of the lattice, while
the discrete change structure provides a minimal element.

\section{Change structures over other structures}
\label{sec:moreStructures}

\subsection{Posets}

A common structure which we want to compute changes on is a poset. For this
section we shall assume that all of our base sets are posets.

\subsubsection{Approximate derivatives}

Firstly, we can define ``approximations'' to derivatives from both sides.

\begin{defn}
  Let $f: A_\cplus \rightarrow B_\cpluss$ be a function. Then a \textit{sup-derivative}
  of $f$ is a function $\supderive{f}$ such that
  $$f(a \cplus \change{a}) \leq f(a) \cpluss \supderive{f}(a, \change{a})$$
  
  Similarly, a \textit{sub-derivative} of $f$ is a function $\subderive{f}$ such that 
  $$f(a \cplus \change{a}) \geq f(a) \cpluss \subderive{f}(a, \change{a})$$

  A function with a sup-derivative is sup-differentiable, and a function with a
  sub-derivative is sub-differentiable.
\end{defn}

Both sup- and sub-derivatives satisfy the chain rule, in the following sense: 
\begin{prop}[Chain rule for sub-derivatives]
  let $f : A_\cplus \rightarrow B_\cpluss$ and $g : B_\cpluss \rightarrow C_\cplusss$ be
  sub-differentiable functions with sub-derivatives $f', g'$ respectively. Then
  $$\derive{g}(f(a)) \circ \derive{f}(a)$$ is a subderivative of $g \circ f$
\end{prop}

Some change structures always have sub- or sup-derivatives: for example every function
into $\mathbb{Z}_1$ is sup-differentiable, and every function into $\mathbb{Z}_2$ is 
sub-differentiable.

\begin{prop}
  If $\derive{f}$ is both a sub- and sup-derivative of $f$, then it is a derivative of $f$.
\end{prop}

Note that this is not the same as saying that if $f$ is both sub- and
sup-differentiable, then it is differentiable. The functions which provide the
sub- and sup-derivatives must coincide for that to be the case. For example,
consider any bounded lattice with the change set $\{ \lambda k . \bot, \lambda k
 . \top \}$. Then any function has a sub-derivative (mapping everything to
 bottom), and a sup-derivative (mapping everything to top), but in most cases
 these are not true derivatives.

Secondly, if the base set of a change structure is a poset, then this gives us a natural
order on the change set.

\begin{defn}[Change order]
  $\change{a} \changeOrder \change{b}$ iff for all $a \in A$ it is the case that $a \cplus \change{a} \leq a \cplus \change{b}$.
\end{defn}

If the change structure is thin, then the order is antisymmetric, and a
full partial order.

Having a monotonic order on the changes is very useful.

\begin{thm}
  Let $f: A \rightarrow B$ be a function, and let $\changeOrder$ be a preorder on $\changes{B}$ such that $\cplus$ is monotonic with
  respect to it. Then let $\supderive{f}$ be a sub-derivative for $f$, and $h: A \times
  \changes{A} \rightarrow \changes{B}$ be a function such that
  $$\supderive{f} \changeOrder h$$
  Then $h$ is also a sup-derivative for $f$.

  Similarly, if $\subderive{f}$ is a sup-derivative for $f$ such that 
  $$h \changeOrder \subderive{f}$$
  Then $h$ is also a sub-derivative for $f$.
\end{thm}
\begin{proof}
  We prove the first case:
  \begin{itemize}
    \item[ ]$\supderive{f}(a, \change{a}) \changeOrder h(a, \change{a})$
    \item[$\Rightarrow$]\{ by monotonicity \}\\
      $f(a) \cplus \supderive{f}(a, \change{a}) \leq f(a) \cplus h(a, \change{a})$
    \item[$\Rightarrow$]\{ sup-derivative property \}\\
      $f(a \cplus \change{a}) \leq f(a) \cplus h(a, \change{a})$
  \end{itemize}

  The proof for the other case is symmetric.
\end{proof}

\begin{thm}[Sandwich lemma]
  \label{thm:sandwich}
  Let $\supderive{f}$ be a sup-derivative for $f$, $\subderive{f}$ be a sub-derivative for $f$, $\changeOrder$ be a preorder on $\changes{B}$ such that $\cplus$ is monotonic with
  respect to it, and $g$ be such that

  $$\supderive{f} \changeOrder g \changeOrder \subderive{f}$$

  Then $g$ is a derivative for $f$.
\end{thm}

In particular, this applies if $g$ and $h$ are themselves derivatives. Moreover,
although the condition of the theorem only requires the bounds to be sub- and
sup-derivatives, the conclusion of the theorem also applies to the bounds, so
they will always be full derivatives as well.

\subsubsection{Ascending and descending change structures}

Having sub- and sup-derivatives alone is not quite enough to ensure that all
functions are differentiable. We need some additional power.

\begin{defn}[Ascending and descending change structures]
  A change structure $A$ is \textit{ascending} if $a \leq b$ implies $a
  \reachOrder b$.

  A change structure $A$ is \textit{descending} if $a \leq b$ implies $b
  \reachOrder a$.
\end{defn}

Intuitively, an ascending change structure is one where you can produce
arbitrary changes that follow the partial order.

\begin{thm}
  Let $A$, $B$ be a change structures, $f: A \rightarrow B$ be a function. Then
  any of the following are sufficient for $f$ to be differentiable.
  \begin{itemize}
    \item $B$ is ascending, and $f$ is sub-differentiable.
    \item $B$ is descending, and $f$ is sup-differentiable.
    \item $B$ is ascending, descending, and has a minimal or maximal element.
  \end{itemize}
\end{thm}

In particular, we can construct change structures where functions are
differentiable by superposing multiple change structures that have some of the
properties that we want.

For example, $\mathbb{Z}_1$ is ascending (and has sup-derivatives), and
$\mathbb{Z}_2$ is descending (and has sub-derivatives), so $\mathbb{Z}_1
\superpose \mathbb{Z}_2 = \mathbb{Z}_3$ has derivatives.

\subsubsection{Maximal and minimal derivatives}

TODO: this section isn't very elegant

If we have a minus operator, then our change structure is complete and all
functions are differentiable. However, there may still be multiple derivatives
for a given function, and we can distinguish them using our order on the change
set.

\begin{defn}[Minus ordering]
  $\cminus_1 \minusOrder \cminus_2$ iff for all $a,b \in A$, $a \cminus_1 b
  \changeOrder a \cminus_2 b$.
\end{defn}

This orders our minus operators according to the size of the changes they
produce. 

\begin{prop}
  If $\cminus_1 \minusOrder \cminus_2$ then
  $\derive{f}_{\cminus_1} \changeOrder \derive{f}_{\cminus_2}$.
\end{prop}

\begin{prop}
  If $\cminus$ is a minimal (maximal) minus operator, then $\derive{f}_\cminus$
  is a minimal (maximal) derivative.
\end{prop}

This then gives us a full characterisation of the derivatives on a complete
change structure.

\begin{thm}[Characterization of derivatives]
\label{thm:derivativeCharacterization}
  Let $A$ be a change structure and $B$ be a complete change structure, let
  $f: A \rightarrow B$ be a function, and let $\subderiveM{f}$ and
  $\supderiveM{f}$ be minimal and maximal derivatives of $f$, respectively.
  Then the derivatives of $f$ are precisely
  the functions $\derive{f}$ such that
  $$\subderiveM{f} \changeOrder \derive{f} \changeOrder \supderiveM{f}$$
\end{thm}
\begin{proof}
  Follows easily from \cref{thm:sandwich} and minimality/maximality.
\end{proof}

This theorem gives us leeway when trying to pick a derivative: we can pick out the
bounds, and that tells us how much ``wiggle room'' we have. This is helpful
because some of the intermediary functions may be much easier to compute than
others, as we will see in \cref{sec:datalogDifferentiability}.

\subsection{Complete partial orders}

Complete partial orders give us a notion of \emph{continuity} of functions.

\begin{defn}[Continuity]
\label{defn:continuity}
  Let $f: A \rightarrow B$ be a function, and $A$ and $B$ be a
  complete lattices. Then $f$ is \textit{continuous} if for any $D \subseteq A$,
  $$f(\bigvee_D a) = \bigvee_D f(a)$$
\end{defn}

In particular, if $\cplus$ is continuous then we can take suprema of sets of derivatives.

\begin{prop}
\label{prop:supDerivatives}
  Let $f: A_\cplus \rightarrow B_\cpluss$ be differentiable, $B$ and
  $\changes{B}$ be cpos, and suppose that $\cpluss$ is continuous
  in its second argument. Then for any set $D$ of derivatives of $f$, $\bigvee
  D$ is also a derivative.
\end{prop}
\begin{proof}
  We show the derivative property directly.
  \begin{itemize}
    \item[ ]$f(a) \cpluss (\bigvee D)(f, \change{a})$
    \item[=]$f(a) \cpluss \bigvee_{\derive{f} \in D} \derive{f}(f, \change{a})$
    \item[=]\{ continuity of $\cpluss$, \cref{defn:continuity} \}\\
      $\bigvee_{\derive{f} \in D} (f(a) \cpluss \derive{f}(f, \change{a}))$
    \item[=]\{ $\derive{f}$ is a derivative \}\\
      $\bigvee_{\derive{f} \in D} f(a \cplus \change{a})$
    \item[=]\{ since $D$ is non-empty, as $f$ is differentiable \}\\
      $f(a \cplus \change{a})$
  \end{itemize}
\end{proof}

We mention another well known fact which will be useful later.

\begin{prop}
\label{prop:monotoneContinuous}
  All monotone functions on cpos which satisfy the Ascending Chain Condition
  (ACC) are continuous.
\end{prop}

\subsection{Lattices}

\begin{defn}
  Let $L$ be a join-semilattice. Then $L_\vee \defeq \cstruct{L}{L}{\vee}$ is a change
  structure on $L$.

  Similarly, if $L$ is a meet-semilattice, then $L_\wedge \defeq \cstruct{L}{L}{\wedge}$ is a change
  structure on $L$.
\end{defn}

\begin{prop}
  All of the following hold
  \begin{itemize}
    \item $L_\vee$ is ascending and has sup-derivatives.
    \item $L_\wedge$ is descending and has sub-derivatives.
    \item $L_\vee \superpose L_\wedge$ has derivatives.
  \end{itemize}
\end{prop}

As usual, the superposition change structure is difficult to work with, since it
consists of sequences of ``upwards'' and ``downwards'' changes. However, it
makes precise the intuition that we can ``glue together'' the ``upwards'' change
structure for joins and the ``downwards'' change structure for meets into a
complete change structure for the full lattice.

\subsection{Boolean algebras}

Boolean algebras give us a much more compact representation for the
superposition of $L_\vee \superpose L_\wedge$.

\begin{prop}
  Let $L$ be a Boolean algebra. Define
  $$L_\superpose \defeq \cstruct{L}{L \times L}{\twist}$$
  where
  $$a \twist (p, q) \defeq (a \vee p) \wedge \neg q$$
  and the monoid operator is
  $$(p, q) \splus (r, s) \defeq ((p \wedge \neg q) \vee r, (q \wedge \neg r) \vee s)$$

  Then $L_\superpose$ is isomorphic to $L_\vee \superpose L_\wedge$.
\end{prop}
\begin{proof}
  First we show that the monoid action property holds:
  \begin{itemize}
    \item[ ]$a \twist \left[(p, q) \splus (r, s)\right]$
    \item[=]$a \twist ((p \wedge \neg q) \vee r, (q \wedge \neg r) \vee s)$
    \item[=]$
      \left(
        a \vee
        \left(
          \left(
            p \wedge \neg q
          \right)
          \vee r
        \right)
      \right)
      \wedge \neg
      \left(
        \left(
          q \wedge \neg r
        \right)
        \vee s
      \right)$
    \item[=]\{ distributing $\vee$ over $\wedge$, applying de Morgan rules \}\\
      $
      \left(
        \left(
          \left(  
            a \vee p
          \right)
          \wedge
          \left(
            a \vee \neg q
          \right)
        \right)
        \vee r
      \right)
      \wedge 
      \left(
        \neg q \vee r
      \right)
      \wedge
      \neg s
      $
    \item[=]\{ un-distributing $\vee$ over $\wedge $\}\\
      $
      \left(
        \left(
          \left(  
            a \vee p
          \right)
          \wedge
          \left(
            a \vee \neg q
          \right)
          \wedge
          \neg q
        \right)
        \vee r
      \right)
      \wedge
      \neg s
      $
    \item[=]\{ $(A \vee B) \wedge B = B$\}\\
      $
      \left(
        \left(
          \left(  
            a \vee p
          \right)
          \wedge
          \neg q
        \right)
        \vee r
      \right)
      \wedge
      \neg s
      $
    \item[=]$a \twist (p, q) \twist (r, s)$
  \end{itemize}


  MPJ: this bit is wrong now that we're using $+$ in the definition of
  superpositions. Might just say it's obvious.
  
  Now $\splus^\ast$ gives an action-preserving injection from $(L \times
  L)^\ast$ to $(L \times L)$. The reverse direction is simply the singleton map.
\end{proof}

We can think of $L_\superpose$ as tracking changes as pairs of ``upwards'' and
``downwards'' changes, where the monoid action simply applies both. This
definition is much more usable than $L_\vee \superpose L_\wedge$, because we
only need to track pairs of changes, rather than sequences, which makes a big
difference in practice.

Boolean algebras also have concrete definitions for maximal and minimal minus
operators.

\begin{prop}
  Let $L$ be a Boolean algebra. Then
  $$a \cminus_\bot b = (a \wedge \neg b, b)$$
  $$a \cminus_\top b = (a, b \wedge \neg a)$$

  define minimal and maximal minus operators.
\end{prop}

In particular, \cref{thm:derivativeCharacterization} gives us bounds for
all the derivatives on Boolean algebras:

\begin{corollary}
\label{cor:booleanCharacterization}
  Let $L$ be a Boolean algebra with the $L_\superpose$ change structure, $A$ be
  a change structure, and $f: A \rightarrow
  L$ a function. Then the derivatives of $f$ are precisely those functions
  $\derive{f}$ such that
  $$
  f(a \cplus \change{a}) \cminus_\bot f(a)
  \changeOrder
  \derive{f}(a, \change{a})
  \changeOrder
  f(a \cplus \change{a}) \cminus_\top f(a)
  $$
\end{corollary}

This makes \cref{thm:derivativeCharacterization} actually usable in practice, as
we have concrete definitions for our bounds (which, again, we will make use of in \cref{sec:datalogDifferentiability}).

\section{Fixpoints}
\label{sec:fixpoints}

\subsection{Incremental computation of fixpoints}

Derivatives give us a technique for computing fixpoints incrementally. Kleene's
fixpoint theorem tells us that fixpoints exist for monotone functions on dcpos, and also gives us
a simple procedure for computing them: start from $\bot$ and apply the function
until there is no change. However, this can be woefully inefficient.

In the Datalog literature, the approach of computing the fixpoint by bottom-up
iteration is called ``naive evaluation''. Naive evaluation has the property that
if it derives a fact at some iteration, it will derive that fact at each
subsequent iteration as well. This is obviously wasteful, and can turn what
should be a linear computation into a quadratic one.

The canonical solution to this problem is ``semi-naive evaluation'', which
attempts to derive only the new facts at each iteration. However, ``semi-naive''
as traditionally presented has some warts, and
the following theorem provides a generalization of it to any differentiable function over a
change structure. We will see the details of how to differentiate Datalog
semantics in \cref{sec:datalogDifferentiability}.

TODO: I had to use the minus to get something generic for $\Delta F_1$ - that's
a bit unsatisfying, is there something better? We can also use ``the change that
maps everything to $F_1$'', if we have that.

\begin{thm}[Incremental computation of iterated function applications]
\label{thm:diffIter}
  Let $L$ be a change structure, $x \in L$, and $f: L \rightarrow L$ be differentiable. Define $F_i$ as follows:

  \begin{eqnarray*}
  F_0 & = & x\\
  \Delta F_0 & = & \mzero \\
  F_1 & = & f(F_0)\\
  \Delta F_1 & = & F_1 \cminus F_0 \\
  F_{i+2} & = & F_{i+1} \cplus \Delta F_{i+2} \\
  \Delta F_{i+2} & = & \derive{f}(F_i, \Delta F_{i+1}) \\
  \end{eqnarray*}

  Then 
  $$f^i(x) = F_i$$
\end{thm}

\begin{proof}
We proceed inductively, proving that $F_{i+2} = f(F_{i+1})$

\begin{itemize}
\item[ ]$F_{i+2}$
\item[=]
$
F_{i+1} \cplus \Delta F_{i+2}
$
\item[=]
$
F_{i+1} \cplus \derive{f}( F_i, \Delta F_{i+1})
$
\item[=] \{ by induction \}\\
$
f(F_i) \cplus \derive{f}(F_i, \Delta F_{i+1})
$
\item[=]
$
f(F_i \cplus \Delta F_{i+1})
$ 
\item[=]
$f(F_{i+1})$
\end{itemize}
\end{proof}

\begin{corollary}[Differential computation of fixed points]
\label{corollary:diffFP}
  Fixed points of differentiable functions which can be calculated by repeated
  function application can also be calculated incrementally.
\end{corollary}

\subsection{Derivatives of fixpoints}
\label{sec:fixpointDerivatives}

The previous section has shown us how to use derivatives to compute fixpoints
more efficiently, but we might also want to take the derivative of a fixpoint
itself. The typical case for this will be where we have some fixed point
$$\fixpoint (\lambda X . F(E, X))$$
and we now wish to apply a change to $E$ and compute
$$\fixpoint (\lambda X . F(E \cplus \change{E}, X))$$

That is, we are applying a change to the function whose fixpoint we are taking.

In Datalog this would allow us to update a recursively defined relation given an
update to a non-recursive dependency of it, such as the base database.

For example, we might want to take the archetypal transitive closure relation

$$tc(x, y) \leftarrow e(x, y)$$
$$tc(x, y) \leftarrow e(x, z), tc(z, y)$$

and update it by changing the base relation $e$.

However, this requires us to have a derivative for the fixpoint operator $\fixpoint$.

\newcommand{\theadjustment}{\operatorname{adjust}}

\begin{defn}
  $$\theadjustment(f, \change{f}) \defeq \lambda\ \change{a} . \derive{\ev}((f,
  \fixpoint_A(f), (\change{f}, \change{a})))$$
\end{defn}

\begin{thm}[Derivatives of fixpoints]
\label{thm:fixpointDiff}
  Let 
  \begin{itemize}
    \item $A$ be a change structure
    \item$\fixpoint_A : (A \rightarrow A) \rightarrow A$ be a fixpoint operator
    \item $f: A \rightarrow A$ be a differentiable function
    \item $\change{f} \in \changes{(A \rightarrow A)}$ be a change to $f$
    \item $\derive{\ev}$ be a derivative of the evaluation map
  \end{itemize}

  Then a change $\change{w} \in \Delta A$ satisfies
  the equation:
  \begin{equation}\label{eqn:fixcondition}
    \change{w} = \theadjustment(f, \change{f})(\change{w})
  \end{equation}
  if and only if $\fixpoint_A(f) \cplus \change{w}$ is a fixpoint of $f \cplus \change{f}$.
  
  In particular, if the operator $\fixpoint$ is defined over $\Delta A$, we can define:
  $$
  \derive{\fixpoint_A}(f, \change{f}) \defeq
  \fixpoint_{\changes{A}}(\theadjustment(f, \change{f}))
  $$
  thus $\fixpoint_A(f) \cplus \derive{\fixpoint_A}(f, \change{f})$ is a fixpoint 
  of $f \cplus \change{f}$.
\end{thm}
\begin{proof}
  Let $\change{w} \in \Delta A$ satisfy \cref{eqn:fixcondition}. Then
  \begin{itemize}
  \item[ ]
    $
    (f \cplus \change{f})(\fixpoint_A(f) \cplus \change{w})
    $
  \item[=]\{ by \cref{prop:incrementalization} \}\\
    $
    f(\fixpoint_A(f))
    \cplus
    \theadjustment(f, \change{f})(\change{w})
    $
  \item[=]\{ rolling the fixpoint and \cref{eqn:fixcondition} \}\\
    $
    \fixpoint_A(f)
    \cplus
    \change{w}
    $
  \end{itemize}
  Hence $\fixpoint_A(f) \cplus \change{w}$ is a fixpoint of $f \cplus \change{f}$. The converse
  follows from reversing the direction of the proof.
\end{proof}

This is not enough to give us a true derivative, because we have only shown 
that $\fixpoint_A(f) \cplus \derive{\fixpoint_A}(f, \change{f})$ computes \emph{a} fixpoint, not necessarily
the same one computed by $\fixpoint_A{(f \cplus \change{f})}$.

However, we can get a true derivative if we are working with least fixpoints.

\begin{thm}[Derivatives of least fixpoints]
\label{thm:leastFixpointDiff}
  Let 
  \begin{itemize}
    \item $A$ be a change structure 
    \item $A, \changes{A}$ be cpos
    \item$\fixpoint_A : (A \rightarrow A) \rightarrow A$ be a differentiable
      least fixpoint operator
    \item $\cplus$ be monotone with respect to the order on $\changes{A}$, and continuous
    \item $f: A \rightarrow A$ be a differentiable function
    \item $\change{f} \in \changes{(A \rightarrow A)}$ be a change to $f$
    \item $\derive{\ev}$ be a derivative of the evaluation map which is monotone
      with respect to $\change{a}$
  \end{itemize}

  Then $\fixpoint_A(f) \cplus \derive{\fixpoint_A}(f, \change{f})$ is a least
  fixpoint of $f \cplus \change{f}$ and therefore $\derive{\fixpoint_A}$ is a derivative of $\fixpoint_A$.
\end{thm}
\begin{proof}
  Suppose that $D$ is a derivative of $\fixpoint_A$, and hence $\fixpoint_A(f) \cplus D(f,
  \change{f})$ is the least fixpoint of $f \cplus \change{f}$. 
  Since $D$ is a derivative of $\fixpoint_A$, it follows that 
  \begin{itemize}
    \item[ ]$\fixpoint_A (f \cplus \change{f})$
    \item[=]$\fixpoint_A(f) \cplus D(f, \change{f})$
    \item[=]$\fixpoint_A(f) \cplus \theadjustment(f, \change{f})(D(f, \change{f}))$
  \end{itemize}

  That is, $\theadjustment(f, \change{f})(D(f, \change{f}))$ is also a
  derivative of $\fixpoint_A$.

  Now, let $$D_{max} \defeq \bigvee \{D \mid D \textrm{ is a derivative of }
  \fixpoint_A \}$$. This exists, since $\changes{A}$ is a complete lattice, and
  is itself a derivative by \cref{prop:supDerivatives}.

  Then, since 
  $\theadjustment(f, \change{f})(D(f, \change{f}))$ is a
  derivative, it is less than $D_{max}$. Hence $D_{max}$ is a prefix point of
  $\theadjustment(f, \change{f})$.

  Since $\derive{\ev}$ is monotone, so is $\theadjustment(f, \change{f})$, and
  hence $\fixpoint_{\changes{A}}(\theadjustment(f, \change{f}))$ is also the
  least prefix point of $\theadjustment(f, \change{f})$, and hence it is less
  than $D_{max}$.

  And so 
  \begin{itemize}
    \item[ ]
      $\fixpoint_A(f \cplus \change{f})$
    \item[$\leq$]\{ $\fixpoint_A$ is a least fixpoint \}\\
      $\fixpoint_A(f) \cplus \derive{\fixpoint_A}(f, \change{f})$
    \item[$\leq$]\{ monotonicity of $\cplus$ \}\\
      $\fixpoint_A(f) \cplus D_{max}(f, \change{f})$
    \item[=]\{ $D_{max}$ is a derivative \}\\
      $\fixpoint_A(f \cplus \change{f})$
  \end{itemize}

  Therefore $\fixpoint_A(f) \cplus \derive{\fixpoint_A}(f, \change{f}) =
  \fixpoint_A(f \cplus \change{f})$, so $\derive{\fixpoint_A}$ is a derivative of $\fixpoint_A$
\end{proof}

The requirements of this theorem may seem onerous. However, usually when we are
working with least fixpoints in a computational setting, we are interested in
being able to compute them by using Kleene's Theorem and iterating from $\bot$.
That means our base set is going to be a cpo satisfying the ACC, and our
functions are going to be monotone. In this setting, it's much easier to satisfy
the conditions.

\begin{prop}
  Let
  \begin{itemize}
    \item $A$ be a complete change structure, and a cpo satisfying the ACC
    \item $\changes{A}$ be ordered according to $\changeOrder$, and a cpo
      satisfying the ACC
    \item $f: A \rightarrow A$ be differentiable and monotone
    \item $\change{f} \in \changes{(A \rightarrow A)}$ be a change to $f$
    \item$f \cplus \change{f}$ be differentiable and monotone, with a monotone derivative.
  \end{itemize}

  TODO: would be nice to have some condition which implied that $\derive{(f
    \cplus \change{f})}$ was monotone rather than having to stipulate it.

  Then
  \begin{itemize}
    \item The least fixpoint operator $\fixpoint_A$ exists for $f$ and $f \cplus
      \change{f}$ and is differentiable
    \item The least fixpoint operator $\fixpoint_{\changes{A}}$ exists for
      $\theadjustment(f, \change{f})$
    \item $\cplus$ is continuous and monotone
    \item There is a monotone derivative of $\ev$
  \end{itemize}

  And therefore $\derive{\fixpoint_A}$ is a derivative of $\fixpoint_A$.
\end{prop}
\begin{proof}
  Largely trivial. Continuity of $\cplus$ follows from
  \cref{prop:monotoneContinuous}; and we take $\derive{\ev}_2$ from
  \cref{prop:evDerivatives} as our derivative of $\ev$, which is monotone 
  since $\derive{(f \cplus \change{f})}$ and $\cplus$ are monotone.
\end{proof}

The requirement that $\changes{A}$ be a cpo satisfying the ACC is still awkward, but is easily
shown to be true on Boolean algebras with $L_\superpose$ (so long as the base
algebra also satisfies the ACC).

Computing the derivative still requires computing a fixpoint (over the change
lattice), but this may still be significantly less expensive than the
alternative ``update strategy'': throw everything away and start
again (which will itself require a fixpoint computation).

\section{Datalog}
\label{sec:datalog}

Datalog is a well-known, simple logic programming language. It is also a textbook
example of the need for incremental computation, as its semantics rely on
bottom-up computation of a least fixpoint.

There is an extensive existing literature on incremental computation of Datalog
(CITES), but the aim of this section is to argue that by viewing the computation
of Datalog semantics as composed of differentiable functions we can give a
bring the machinery that we've developed so far to bear, and give an elegant,
flexible account of incremental evaluation.

Firstly, however, we must show that we can see the semantics of Datalog in terms
of elements that we know how to handle: Boolean algebras and fixpoints.

\subsection{Denotational semantics}

Datalog is usually given a logical semantics wherein we look for models that
satisfy the program. We will instead give a simple denotational semantics that treats a Datalog
program as denoting a family of relations.

One obstacle to this approach is giving a denotation to negations. We adopt the
closed-world assumption to solve this.

\begin{defn}[Closed-world assumption and negation]
  There exists a universal relation $\universalRel$.
  
  Negation on relations is defined as $$\neg R \defeq \universalRel \setminus R$$
\end{defn}

This makes $\Rel$ into a Boolean algebra.

\begin{defn}[Term semantics]
  A Datalog term $T$ denotes a function from its free relation variables to
  $\Rel$, $\denote{\_} : \Term \rightarrow \Rel^n \rightarrow \Rel$
  \begin{eqnarray*}
    \denote{R_i}(\overline{X}) &\defeq& \overline{X}[i] \\
    \denote{T \wedge U}(\overline{X}) &\defeq& \denote{T}(\overline{X}) \cap  \denote{U}(\overline{X})\\
    \denote{T \vee U}(\overline{X}) &\defeq& \denote{T}(\overline{X}) \cup  \denote{U}(\overline{X})\\
    \denote{\exists x. T}(\overline{X}) &\defeq& \pi_x(\denote{T}(\overline{X}))\\
    \denote{\neg T}(\overline{X}) &\defeq& \neg \denote{T}(\overline{X})\\
    \denote{x=y}(\overline{X}) &\defeq& \Delta(x, y) \text{ where } \Delta \text{ is the diagonal relation }
  \end{eqnarray*}
\end{defn}

Since $\Rel$ is a Boolean algebra, and so is $\Rel^n$, the denotation
functions of terms are functions between Boolean algebras.

\begin{defn}[Immediate consequence operator]
  Given a program $\mathcal{P} = \overline{P}$, the immediate consequence operator $\consq: \Rel^n \rightarrow \Rel^n$ is defined as follows:
  $$\consq(\overline{R}) = \overline{\denote{P_i}(\overline{R})}$$
\end{defn}

That is, given a value for the program, we pass in all the relations
to the denotation of each predicate, to get a new product of relations.

\begin{defn}[Program semantics]
  The semantics of a program $\mathcal{P}$ is defined to be 
  $$\denote{\mathcal{P}} \defeq \lfp_{\Rel^n}(\consq)$$
  and may be calculated by repeated application of $\consq$ to $\bot$.
\end{defn}

Whether or not this program semantics exists will depend on whether the fixpoint
exists. Typically this is ensured by constraining the program such that $\consq$
is monotone. However, we will remain agnostic on this front - we are merely
interested in calculating the semantics faster if it exists.

\subsection{Differentiability of Datalog semantics}
\label{sec:datalogDifferentiability}

In order to apply the machinery we have developed, we need the semantics $\denote{\_}$ to
be differentiable. However, it is a function on Boolean algebras, and we know
that the $L_\superpose$ change structure is complete, so in fact we know that
$\denote{\_}$ must be differentiable.

However, this does not mean that we have a \emph{good} derivative for
$\denote{\_}$. The derivative that we know we have for complete change structures
is quite bad:
$$\derive{f}(a, \change{a}) = f(a \cplus \change{a}) \cminus f(a)$$
Naively computed, this requires \emph{more} work than evaluating $f(a \cplus \change{a})$ directly!

However, \cref{cor:booleanCharacterization} gives us a range of derivatives to
choose from, and we can optimize within that range to find one that satisfies
our constraints.

In the case of Datalog, the change ordering on the change structure also
corresponds to the size of the derivative as a pair of relations. The minimal
derivative contains precisely the elements that are newly added or removed,
whereas the maximal derivative contains all the elements that have \emph{ever}
been added or removed. This means that \cref{cor:booleanCharacterization} allows
us to \emph{approximate} the most precise derivative while still being
guaranteed that the result is sound.\footnote{The idea of using an approximation
to the precise derivative, and a soundness condition, appears in \textcite{bancilhon1986amateur}.}

There is also the question of how to compute the derivative. Fortunately, the
maximal and minimal minus operators are actually representable as pairs of terms
in our Datalog, and so we can compute the derivative via a pair of terms that
satisfy those bounds, allowing us to reuse our machinery for evaluating Datalog
terms.\footnote{Indeed, if this process is occurring in an optimizing compiler,
  the derivative terms can themselves be optimized as well. This is likely to be
  beneficial, since as we will see the initial terms may be quite complex.}

This does give us additional constraints that the derivative terms must satisfy:
for example, we may need to preserve safety or range restriction in order to be
able to evaluate them; and we may wish to pick terms that will be easy or cheap
for our evaluation engine to evaluate, even if the results are larger.

The upshot of these considerations is that the optimal choice of derivatives is likely
to be quite dependent on the precise variant of Datalog being evaluated, and the
specifics of the evaluation engine. Here is one possibility.\footnote{These are
  the rules actually in use at Semmle. We arrived at them by starting with the
  minimal derivative and then simplifying and weakening it while preserving the
  bound given by the maximal derivative.}

\newcommand{\bothdiff}{\diamond}
\begin{thm}[Concrete Datalog term derivatives]
\label{thm:concreteDatalog}
  We give two mutually recursive definitions,
  $\updiff: \Term \rightarrow \changes{\Rel^n} \rightarrow \changes{Rel}$ and
  $\downdiff: \Term \rightarrow \changes{\Rel^n} \rightarrow \changes{Rel}$, such
  that $\updiff \times \downdiff$ is a derivative for Datalog term semantics.
  
  Let $$\diamond X \defeq X \twist (\updiff X, \downdiff X)$$ in the following.

  TODO: make the references to the argument changes more obvious
  
  \begin{eqnarray*}
  \updiff\bot & \defeq & \bot\\
  \updiff\top & \defeq & \bot\\
  \updiff R & \defeq & \updiff R\\
  \updiff(T\vee U) & \defeq & \updiff T \vee \updiff U\\
  \updiff(T\wedge U) & \defeq & (\updiff T\wedge \bothdiff U) 
                           \vee 
                           (\updiff U \wedge \bothdiff T)\\
  \updiff(\neg T) & \defeq & \downdiff T\\
  \updiff(\exists x.T) & \defeq & \exists x.\updiff T
  \end{eqnarray*}

  \begin{eqnarray*}
  \downdiff\bot & \defeq & \bot\\
  \downdiff\top & \defeq & \bot\\
  \downdiff R & \defeq & \downdiff R\\
  \downdiff(T\vee U) & \defeq & (\downdiff T \wedge \neg \bothdiff U) 
                           \vee 
                           (\downdiff U \wedge \neg \bothdiff T)\\
  \downdiff(T\wedge U) & \defeq & (\downdiff T\wedge U) \vee (T \wedge \downdiff U)\\
  \downdiff(\neg T) & \defeq & \updiff T\\
  \downdiff(\exists x.T) & \defeq & \exists x.\downdiff T \wedge \neg \exists x.\bothdiff T
  \end{eqnarray*}
\end{thm}
\begin{proof}
  Long, tedious structural induction. Maybe in an appendix?
\end{proof}

There is a symmetry between the cases for $\wedge$ and $\vee$ between $\updiff$
and $\downdiff$, but the cases for $\exists$ look quite different. 
This is because we have chosen a dialect of Datalog without a primitive universal quantifier.
If we did have one, its cases would be dual to those for $\exists$, namely:
\begin{eqnarray*}
\updiff(\forall x.T) & = & \exists x. \updiff T \wedge \forall x. \bothdiff T\\
\downdiff(\forall x.T) & = & \forall x. \downdiff T
\end{eqnarray*}

We can easily extend a derivative for the term semantics to a derivative for $\consq$.

\begin{corollary}
\label{corollary:consqDiff}
  $\consq$ is differentiable.
\end{corollary}

\subsection{Differential evaluation of Datalog}

Putting this together, we get two results.

\begin{thm}[Differential evaluation of Datalog semantics]
\label{thm:diffEval}
  Datalog program semantics can be evaluated incrementally.
\end{thm}
\begin{proof}
  Corollary of \cref{corollary:diffFP} and \cref{corollary:consqDiff}.
\end{proof}

This is a generalization of semi-naive evaluation.

\begin{thm}[Differential update of Datalog semantics]
\label{thm:diffUpdate}
  Datalog program semantics can be incrementally updated with changes to non-recursive relations.
\end{thm}
\begin{proof}
  Corollary of \cref{thm:leastFixpointDiff} and \cref{corollary:consqDiff}.
\end{proof}

This is a generalization of the view-update problem, including recursive relations.

\subsection{Extensions to Datalog}

Our formulation of Datalog term semantics and differential evaluation is quite
generic and composable, so it is relatively easy to extend the language with new
term constructs.

A new term construct must have:
\begin{itemize}
  \item An interpretation as a function on its free relation variables.
  \item An implementation of $\updiff$ and $\downdiff$.
\end{itemize}

These are very easy conditions - the former is needed for the construct to even
make sense, and the latter can always be satisfied by using the maximal or
minimal derivative. Although they are very bad derivatives, having them
available as options is very helpful. It means that even if we have a term
construct which we cannot find a good derivative for, we only lose incremental
evaluation for those subterms, and not for anything else.

Note that this does not say anything about monotonicity. New term constructs may
be required to be monotonic if they are to participate in recursion, but we are
interested in derivatives even for non-monotonic terms because they allow us to
use \cref{thm:diffUpdate}.

This is important in practice for Semmle's variant of Datalog, which includes
aggregation and other primitives with interesting derivatives.

\section{Related work}

\subsection{Change structures}

\subsubsection{Change structures}

MPJ: the title of this will be less redundant once we call ours something else

The seminal paper in this area is \textcite{cai2014changes}. We use the notions
defined in that excellent paper heavily, but we deviate in two regards: the use of
dependent types, and the nature of function changes.

These two issues are linked, because part of the reason that \citeauthor{cai2014changes} need
dependently typed changes is in order to handle their notion of function
changes.

Our notion of function changes is different to \citeauthor{cai2014changes}'s,
because we consider changes of functions to be what \citeauthor{cai2014changes} describe as
\textit{pointwise changes}.~\footcite[See][section 2.2]{cai2014changes} As they point out, you can reconstruct their
function changes from pointwise changes and derivatives, so the two formulations
are equivalent. This gives us the following correspondences:
\begin{itemize}
  \item Our \textit{derivatives of the
      evaluation map} are \citeauthor{cai2014changes}'s \textit{function changes}
  \item Our \textit{function changes} are \citeauthor{cai2014changes}'s \textit{pointwise differences}
\end{itemize}

Our equivalent to \citeauthor{cai2014changes}'s ``Incrementalization'' lemma
(\cref{prop:incrementalization}) is an important lemma for us as well - it is used
crucially in \cref{sec:fixpointDerivatives}. However, the incremental computation part of
the lemma simply follows from the fact that we are using the derivative of the
evaluation map - any derivative will do. It is then a further fact that we can
find actual definitions for this derivative (\cref{prop:evDerivatives}).

\citeauthor{cai2014changes} assert that the reason they use their function changes instead of pointwise
changes as the primitive notion is that a function change has access to more
information, and so is easier to optimize. In contrast, we have not found pointwise changes to be
significantly harder to work with in practice, or more difficult to compute (at least in our implementations
in Datalog). Furthermore, pointwise changes are simpler from a theoretical point of view, and correspond to the
exponentials that we get from the categorical equivalence with $\cat{PreOrd}$. 

However, this is largely an issue of presentation: if it turned out that it was
significantly easier to work with function changes rather than pointwise
changes, then there is nothing preventing a ``plugin'' for \citeauthor{cai2014changes}'s system providing the
ability to create either or both.

The equivalence of our presentations means that our work should be compatible
with ILC. In particular, this is likely to be of interest for languages like
Datafun (see \cref{sec:embeddingDatalog} below) which embed Datlog or logic programming behaviour within a broader
programming context.

Our function changes do not need to be dependently typed, so we do not need a
dependently typed formulation for the cases we have been considering. However,
if we want to extend this formulation to differential geometry we will need
dependently typed changes, since the type of the tangent space is
dependent on the point at which the tangents are taken.

In fact, a slightly different dependently-typed formulation seems promising, since it would allow us
to draw out a 2-categorical interpretation of change structures. For example, if
we consider our base set as a category, with changes $\Delta_a^b$ between $a$
and $b$ as morphisms between $a$ and $b$, then a function is
differentiable iff it is a functor on that category (i.e. provides a change in
$\Delta_{f(a)}^{f(b)}$). 

While this is a promising direction, it wasn't necessary for the material in
this paper, and would have complicated the exposition significantly, so we opted
to leave it for future work.

\subsubsection{S-acts}

S-acts and their categorical structure have received a fair amount of attention
over the years (\textcite{kilp2000monoids} is a good
overview). However, there is a key difference between our $\cat{CStruct}$ and the category of
S-acts $\cat{SAct}$: the objects of $\cat{SAct}$ all maintain the same monoid
structure, whereas we are interested in changing both the base set \emph{and} the structure of the act.

There are similarities: if we compare the definition of a $\cat{SAct}$ ``act-preserving''
homeomorphism \footcite[See][]{kilp2000monoids} we can see that the structure is
quite similar to the definition of differentiability:

$$f(a \splus s) = f(a) \splus s$$

as opposed to

$$f(a \cplus s) = f(a) \cplus \derive{f}(a, s)$$

That is, we use $\derive{f}$ to transform the action element into the new
monoid, whereas in $\cat{SAct}$ it simply remains the same.

In fact, $\cat{SAct}$ is a subcategory of $\cat{CStruct}$, where we only
consider change structures with change set $S$, and the only functions are those
whose derivative is $\lambda a. \lambda d. d$.

\subsubsection{Derivatives of fixpoints}

\textcite{arntz2017fixpoints} gives a derivative operator for fixpoints. However,
it uses the definition of function changes from Cai et al., whereas
we have a different notion of function changes, so the result is unfortunately
inapplicable. In addition, we prove our result for all functions, not just
monotone functions.

\subsection{Datalog}

\subsubsection{Incremental evaluation}

The earliest explication of semi-naive evaluation as a derivative process
appears in \textcite{bancilhon1986naive}. The idea of using an approximate derivative
and the requisite soundness condition appears as a throwaway comment in
\textcite{bancilhon1986amateur}, and as far as we know nobody has since
developed that approach. 

As far as we know, traditional semi-naive is generally considered the state of
the art in incremental Datalog evaluation, and there are no strategies that
accommodate additional (recursive) language features such as aggregates.

\subsubsection{Incremental updates}

There is existing literature on incremental updates to relational algebra
expressions. In particular \textcite{griffin1997improved} following
\textcite{qian1991incremental} shows the essential insight that it is necessary to
track both an ``upwards'' and a ``downwards'' difference, and produces a set of
rules that look quite similar to those we derive in \cref{thm:concreteDatalog}.

Where our presentation improves over \citeauthor{griffin1997improved} is mainly in
the genericity of the presentation. Our machinery works for a wider variety of
algebraic structures, and it is clear how the parts of the proof work together
to produce the result. In addition, it is easy to see how to extend the proofs
to cover additional language constructs.

\citeauthor{griffin1997improved} are interested in \emph{minimal} changes. These correspond to
minimal derivatives in the sense of \cref{cor:booleanCharacterization}. However,
while minimality (or proximity to minimality) is desirable (since it decreases
the size of the derivatives as relations), it is important to be able to trade
it off against other considerations. For example, at
Semmle we use the derivatives given in \cref{thm:concreteDatalog}, which are not minimal.

There are some inessential points of difference as well: we work on Datalog,
rather than relational algebra; and we use set semantics rather than bag
semantics. This is largely a matter of convenience: Datalog is an easier
language to work with, and set semantics allows a much wider range of valid
simplifications. However, all the same machinery applies to relational algebra
with bag semantics, it is simply necessary to produce a valid version of \cref{thm:concreteDatalog}.

We also solve the problem of updating \emph{recursive} expressions. As far as we
know, this is unsolved in general. Most of the attempts to solve it have
focussed on Datalog rather than relational algebra, since Datalog is designed to
make heavy use of recursion. 

\textcite{gupta1995maintenance} has a good summary of the approaches so far.

WRITE MORE HERE

\subsubsection{Embedding Datalog}
\label{sec:embeddingDatalog}

Datafun (\textcite{arntz2016datafun}) is a functional programming language that embeds
Datalog, allowing significant improvements in genericity, such as the use of
higher-order functions. Since we have directly defined a change structure and
derivative operator for Datalog, our work could be used as a ``plugin'' in the sense
of \citeauthor{cai2014changes}, allowing Datafun to compute its internal fixpoints
incrementally, but also allowing Datafun expressions to be fully incrementally updated.

\printbibliography

\end{document}
